{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c32a665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_era5_lai_whole.py\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f094f5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ERA5LAIWholeWorld(Dataset):\n",
    "    \"\"\"\n",
    "    One YEAR, 24 items (one per 15-day sample). Each __getitem__ returns:\n",
    "        X: (3, H, W)  -> [ssrd, t2m, tp]  (raw/anom/z as chosen)\n",
    "        y: (1, H, W)  -> LAI (raw or anomaly)\n",
    "        mask: (1, H, W) boolean (True where y is valid)\n",
    "        meta: dict with 'year' and 'sample'\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        year: int,\n",
    "        era5_mode=\"anom\",   # \"raw\" | \"anom\" | \"z\"\n",
    "        lai_mode=\"raw\",     # \"raw\" | \"anom\"\n",
    "        paths=None,\n",
    "        engine=\"netcdf4\",\n",
    "        robust_nan=True,\n",
    "        sample_indices=None,  # default: all 0..23\n",
    "    ):\n",
    "        assert era5_mode in (\"raw\",\"anom\",\"z\")\n",
    "        assert lai_mode in (\"raw\",\"anom\")\n",
    "        self.year = int(year)\n",
    "        self.era5_mode = era5_mode\n",
    "        self.lai_mode = lai_mode\n",
    "        self.engine = engine\n",
    "        self.robust_nan = robust_nan\n",
    "        self.sample_indices = list(range(24)) if sample_indices is None else list(sample_indices)\n",
    "\n",
    "        # default paths (override with 'paths' dict)\n",
    "        default_paths = {\n",
    "            \"era5_root\": \"/ptmp/mp002/ellis/lai\",\n",
    "            \"era5_anom_dir\": \"/ptmp/mp040/outputdir/era5/anom\",   # your anomalies/z-scores\n",
    "            \"lai_root\": \"/ptmp/mp002/ellis/lai/lai\",\n",
    "            \"lai_tmpl\": \"LAI.1440.720.{year}.nc\",\n",
    "            \"lai_anom_dir\": \"/ptmp/mp002/ellis/lai/anom\",         # change if different\n",
    "        }\n",
    "        self.paths = default_paths if paths is None else {**default_paths, **paths}\n",
    "\n",
    "        # open all arrays for that year\n",
    "        self._open_year()\n",
    "\n",
    "    def _open_da(self, path, varname):\n",
    "        ds = xr.open_dataset(path, engine=self.engine)\n",
    "        da = ds[varname]\n",
    "        if self.robust_nan:\n",
    "            fv = da.attrs.get(\"_FillValue\", None)\n",
    "            if fv is not None:\n",
    "                da = da.where(da != fv)\n",
    "        # north-up\n",
    "        lat_name = \"lat\" if \"lat\" in da.coords else \"latitude\"\n",
    "        da = da.sortby(lat_name)\n",
    "        # attach sample coord\n",
    "        if \"time\" in da.dims and da.sizes[\"time\"] == 24:\n",
    "            da = da.assign_coords(sample=(\"time\", np.arange(24)))\n",
    "        return da\n",
    "\n",
    "    def _open_year(self):\n",
    "        y = self.year\n",
    "\n",
    "        # ERA5 inputs\n",
    "        if self.era5_mode == \"raw\":\n",
    "            f_ssrd = os.path.join(self.paths[\"era5_root\"], \"ssrd\", f\"ssrd.15daily.fc.era5.1440.720.{y}.nc\")\n",
    "            f_t2m  = os.path.join(self.paths[\"era5_root\"], \"t2m\",  f\"t2m.15daily.an.era5.1440.720.{y}.nc\")\n",
    "            f_tp   = os.path.join(self.paths[\"era5_root\"], \"tp\",   f\"tp.15daily.fc.era5.1440.720.{y}.nc\")\n",
    "            self.ssrd = self._open_da(f_ssrd, \"ssrd\")\n",
    "            self.t2m  = self._open_da(f_t2m,  \"t2m\")\n",
    "            self.tp   = self._open_da(f_tp,   \"tp\")\n",
    "        else:\n",
    "            suffix = \"anom\" if self.era5_mode == \"anom\" else \"z\"\n",
    "            base = self.paths[\"era5_anom_dir\"]\n",
    "            self.ssrd = self._open_da(os.path.join(base, f\"ssrd_{suffix}_{y}.nc\"), f\"ssrd_{suffix}\")\n",
    "            self.t2m  = self._open_da(os.path.join(base, f\"t2m_{suffix}_{y}.nc\"),  f\"t2m_{suffix}\")\n",
    "            self.tp   = self._open_da(os.path.join(base, f\"tp_{suffix}_{y}.nc\"),   f\"tp_{suffix}\")\n",
    "\n",
    "        # LAI target\n",
    "        lai_file = os.path.join(self.paths[\"lai_root\"], self.paths[\"lai_tmpl\"].format(year=y))\n",
    "        lai_var = self._infer_var(lai_file)\n",
    "        self.lai = self._open_da(lai_file, lai_var)\n",
    "\n",
    "        if self.lai_mode == \"anom\":\n",
    "            # use your saved LAI anomalies per year if available\n",
    "            lai_anom_dir = self.paths.get(\"lai_anom_dir\", self.paths[\"lai_root\"])\n",
    "            lai_anom_file = os.path.join(lai_anom_dir, f\"LAI_anom_{y}.nc\")\n",
    "            if os.path.exists(lai_anom_file):\n",
    "                self.lai = self._open_da(lai_anom_file, \"LAI_anom\")\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"LAI anomaly file not found: {lai_anom_file}\")\n",
    "\n",
    "        # dims/coords\n",
    "        self.lat_name = \"lat\" if \"lat\" in self.lai.coords else \"latitude\"\n",
    "        self.lon_name = \"lon\" if \"lon\" in self.lai.coords else \"longitude\"\n",
    "\n",
    "        # sanity checks\n",
    "        for da, name in [(self.ssrd,\"ssrd\"),(self.t2m,\"t2m\"),(self.tp,\"tp\")]:\n",
    "            assert \"time\" in da.dims and da.sizes[\"time\"] == 24, f\"{name} expects 24 samples\"\n",
    "            assert da.sizes[self.lat_name] == self.lai.sizes[self.lat_name]\n",
    "            assert da.sizes[self.lon_name] == self.lai.sizes[self.lon_name]\n",
    "        assert \"time\" in self.lai.dims and self.lai.sizes[\"time\"] == 24\n",
    "\n",
    "    def _infer_var(self, nc_path):\n",
    "        with xr.open_dataset(nc_path, engine=self.engine) as ds:\n",
    "            for v in ds.data_vars:\n",
    "                if ds[v].ndim >= 2:\n",
    "                    return v\n",
    "        raise RuntimeError(f\"No data variable in {nc_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_indices)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s = self.sample_indices[i]\n",
    "        # features (H,W) each\n",
    "        x_ssrd = self.ssrd.isel(time=s).values\n",
    "        x_t2m  = self.t2m .isel(time=s).values\n",
    "        x_tp   = self.tp  .isel(time=s).values\n",
    "        # target\n",
    "        y_lai  = self.lai .isel(time=s).values  # may contain NaNs over ocean\n",
    "\n",
    "        # stack to tensors, channel-first\n",
    "        X = np.stack([x_ssrd, x_t2m, x_tp], axis=0)              # (3,H,W)\n",
    "        y = np.expand_dims(y_lai, axis=0)                       # (1,H,W)\n",
    "        mask = ~np.isnan(y)                                     # valid target pixels\n",
    "\n",
    "        # features: fill NaNs with 0 (or choose another fill)\n",
    "        X = np.nan_to_num(X, nan=0.0)\n",
    "\n",
    "        X = torch.from_numpy(X.astype(np.float32))\n",
    "        y = torch.from_numpy(np.nan_to_num(y, nan=0.0).astype(np.float32))\n",
    "        mask = torch.from_numpy(mask.astype(np.bool_))\n",
    "        meta = {\"year\": self.year, \"sample\": int(s)}\n",
    "        return X, y, mask, meta\n",
    "\n",
    "# Loss helper (same as before)\n",
    "def masked_mse_loss(pred, target, mask):\n",
    "    diff2 = (pred - target) ** 2\n",
    "    diff2 = diff2 * mask.float()\n",
    "    denom = mask.float().sum().clamp_min(1.0)\n",
    "    return diff2.sum() / denom\n",
    "\n",
    "def collate_keep_meta(batch):\n",
    "    \"\"\"Custom collate so that 'meta' stays a list of dicts (not collated into tensors).\"\"\"\n",
    "    Xs, ys, masks, metas = zip(*batch)\n",
    "    return torch.stack(Xs), torch.stack(ys), torch.stack(masks), list(metas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05334306",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "ds = ERA5LAIWholeWorld(\n",
    "    year=1990,\n",
    "    era5_mode=\"raw\",   # or \"raw\"/\"z\"\n",
    "    lai_mode=\"raw\",     # or \"anom\"\n",
    "    paths={\n",
    "        \"era5_root\": \"/ptmp/mp002/ellis/lai\",\n",
    "        \"era5_anom_dir\": \"/ptmp/mp040/outputdir/era5/anom\",\n",
    "        \"lai_root\": \"/ptmp/mp002/ellis/lai/lai\",\n",
    "        \"lai_tmpl\": \"LAI.1440.720.{year}.nc\",\n",
    "        # \"lai_anom_dir\": \"/ptmp/mp002/ellis/lai/anom\",  # if using LAI anomalies\n",
    "    },\n",
    ")\n",
    "\n",
    "loader = DataLoader(ds, batch_size=1, shuffle=False)  # batch_size=1 since each item is full globe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97d40529",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mp186/conda-envs/lai_env/lib/python3.12/site-packages/juliacall/__init__.py:61: UserWarning: torch was imported before juliacall. This may cause a segfault. To avoid this, import juliacall before importing torch. For updates, see https://github.com/pytorch/pytorch/issues/78829.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected IPython. Loading juliacall extension. See https://juliapy.github.io/PythonCall.jl/stable/compat/#IPython\n"
     ]
    }
   ],
   "source": [
    "from pysr import PySRRegressor\n",
    "\n",
    "model = PySRRegressor(\n",
    "    maxsize=20,\n",
    "    niterations=40,  # < Increase me for better results\n",
    "    binary_operators=[\"+\", \"*\"],\n",
    "    unary_operators=[\n",
    "        \"cos\",\n",
    "        \"exp\",\n",
    "        \"sin\",\n",
    "        \"inv(x) = 1/x\",\n",
    "        # ^ Custom operator (julia syntax)\n",
    "    ],\n",
    "    extra_sympy_mappings={\"inv\": lambda x: 1 / x},\n",
    "    # ^ Define operator for SymPy as well\n",
    "    elementwise_loss=\"loss(prediction, target) = (prediction - target)^2\",\n",
    "    # ^ Custom loss function (julia syntax)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2394d95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mp186/conda-envs/lai_env/lib/python3.12/site-packages/pysr/sr.py:2811: UserWarning: Note: it looks like you are running in Jupyter. The progress bar will be turned off.\n",
      "  warnings.warn(\n",
      "/u/mp186/conda-envs/lai_env/lib/python3.12/site-packages/pysr/sr.py:2265: UserWarning: Note: you are running with more than 10,000 datapoints. You should consider turning on batching (https://ai.damtp.cam.ac.uk/pysr/options/#batching). You should also reconsider if you need that many datapoints. Unless you have a large amount of noise (in which case you should smooth your dataset first), generally < 10,000 datapoints is enough to find a functional form with symbolic regression. More datapoints will lower the search speed.\n",
      "  warnings.warn(\n",
      "Compiling Julia backend...\n",
      "[ Info: Note: you are running with more than 10,000 datapoints. You should consider turning on batching (`options.batching`), and also if you need that many datapoints. Unless you have a large amount of noise (in which case you should smooth your dataset first), generally < 10,000 datapoints is enough to find a functional form.\n",
      "[ Info: Started!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Expressions evaluated per second: 0.000e+00\n",
      "Progress: 0 / 1240 total iterations (0.000%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 2.310e+00\n",
      "Progress: 1 / 1240 total iterations (0.081%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           2.170e+00  0.000e+00  y = 1.1143\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 1.440e+03\n",
      "Progress: 17 / 1240 total iterations (1.371%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           2.170e+00  0.000e+00  y = 1.1143\n",
      "3           2.115e+00  1.287e-02  y = cos(inv(x₀))\n",
      "4           2.098e+00  8.210e-03  y = cos(sin(inv(x₀)))\n",
      "5           1.806e+00  1.495e-01  y = (x₀ * 0.06683) + 0.40192\n",
      "6           1.663e+00  8.265e-02  y = (exp(-1.1497) * x₂) * 0.068652\n",
      "10          1.559e+00  1.613e-02  y = sin(x₁ * -0.080598) + cos(sin(x₀ + 1.9364))\n",
      "12          1.427e+00  4.441e-02  y = sin(x₁ * -0.079955) + cos(sin((x₀ * 0.15763) + 0.49021...\n",
      "                                      ))\n",
      "13          1.379e+00  3.395e-02  y = exp(sin(x₁ * -0.030941) + cos(sin((x₀ * 0.12953) + 0.9...\n",
      "                                      6536)))\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 1.750e+03\n",
      "Progress: 30 / 1240 total iterations (2.419%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           2.170e+00  0.000e+00  y = 1.1143\n",
      "3           2.115e+00  1.287e-02  y = cos(inv(x₀))\n",
      "4           2.098e+00  8.210e-03  y = cos(sin(inv(x₀)))\n",
      "5           1.806e+00  1.495e-01  y = (x₀ * 0.06683) + 0.40192\n",
      "6           1.663e+00  8.265e-02  y = (exp(-1.1497) * x₂) * 0.068652\n",
      "10          1.559e+00  1.613e-02  y = sin(x₁ * -0.080598) + cos(sin(x₀ + 1.9364))\n",
      "12          1.427e+00  4.441e-02  y = sin(x₁ * -0.079955) + cos(sin((x₀ * 0.15763) + 0.49021...\n",
      "                                      ))\n",
      "13          1.379e+00  3.395e-02  y = exp(sin(x₁ * -0.030941) + cos(sin((x₀ * 0.12953) + 0.9...\n",
      "                                      6536)))\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 1.970e+03\n",
      "Progress: 45 / 1240 total iterations (3.629%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           2.170e+00  0.000e+00  y = 1.1143\n",
      "3           1.663e+00  1.331e-01  y = x₂ * 0.021745\n",
      "4           1.663e+00  1.645e-05  y = x₂ * sin(0.021745)\n",
      "5           1.663e+00  1.615e-05  y = x₂ * sin(sin(0.021745))\n",
      "10          1.559e+00  1.290e-02  y = sin(x₁ * -0.080598) + cos(sin(x₀ + 1.9364))\n",
      "12          1.427e+00  4.441e-02  y = sin(x₁ * -0.079955) + cos(sin((x₀ * 0.15763) + 0.49021...\n",
      "                                      ))\n",
      "13          1.379e+00  3.395e-02  y = exp(sin(x₁ * -0.030941) + cos(sin((x₀ * 0.12953) + 0.9...\n",
      "                                      6536)))\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 2.190e+03\n",
      "Progress: 60 / 1240 total iterations (4.839%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           2.170e+00  0.000e+00  y = 1.1143\n",
      "3           1.663e+00  1.331e-01  y = x₂ * 0.021745\n",
      "4           1.663e+00  1.645e-05  y = x₂ * sin(0.021745)\n",
      "5           1.663e+00  1.615e-05  y = x₂ * sin(sin(0.021745))\n",
      "10          1.559e+00  1.290e-02  y = sin(x₁ * -0.080598) + cos(sin(x₀ + 1.9364))\n",
      "12          1.427e+00  4.441e-02  y = sin(x₁ * -0.079955) + cos(sin((x₀ * 0.15763) + 0.49021...\n",
      "                                      ))\n",
      "13          1.379e+00  3.395e-02  y = exp(sin(x₁ * -0.030941) + cos(sin((x₀ * 0.12953) + 0.9...\n",
      "                                      6536)))\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 2.720e+03\n",
      "Progress: 76 / 1240 total iterations (6.129%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           2.170e+00  0.000e+00  y = 1.1143\n",
      "3           1.649e+00  1.374e-01  y = x₂ * 0.019943\n",
      "5           1.477e+00  5.510e-02  y = (x₂ + x₀) * 0.021745\n",
      "12          1.427e+00  4.922e-03  y = sin(x₁ * -0.079955) + cos(sin((x₀ * 0.15763) + 0.49021...\n",
      "                                      ))\n",
      "13          1.379e+00  3.395e-02  y = exp(sin(x₁ * -0.030941) + cos(sin((x₀ * 0.12953) + 0.9...\n",
      "                                      6536)))\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 2.740e+03\n",
      "Progress: 96 / 1240 total iterations (7.742%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           2.170e+00  0.000e+00  y = 1.1143\n",
      "3           1.649e+00  1.374e-01  y = x₂ * 0.019943\n",
      "5           1.477e+00  5.510e-02  y = (x₂ + x₀) * 0.021745\n",
      "12          1.427e+00  4.922e-03  y = sin(x₁ * -0.079955) + cos(sin((x₀ * 0.15763) + 0.49021...\n",
      "                                      ))\n",
      "13          1.379e+00  3.395e-02  y = exp(sin(x₁ * -0.030941) + cos(sin((x₀ * 0.12953) + 0.9...\n",
      "                                      6536)))\n",
      "17          1.345e+00  6.289e-03  y = cos(sin((x₀ * 0.13406) + inv(cos(exp(x₁ * 0.0024216)))...\n",
      "                                      )) + sin(x₁ * -0.079539)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 2.560e+03\n",
      "Progress: 103 / 1240 total iterations (8.306%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           2.170e+00  0.000e+00  y = 1.1143\n",
      "3           1.649e+00  1.374e-01  y = x₂ * 0.019943\n",
      "5           1.477e+00  5.510e-02  y = (x₂ + x₀) * 0.021745\n",
      "10          1.473e+00  5.286e-04  y = sin(x₁ * -0.079955) + cos(sin(x₀ * 0.15763))\n",
      "12          1.427e+00  1.590e-02  y = sin(x₁ * -0.079955) + cos(sin((x₀ * 0.15763) + 0.49021...\n",
      "                                      ))\n",
      "13          1.379e+00  3.395e-02  y = exp(sin(x₁ * -0.030941) + cos(sin((x₀ * 0.12953) + 0.9...\n",
      "                                      6536)))\n",
      "17          1.345e+00  6.289e-03  y = cos(sin((x₀ * 0.13406) + inv(cos(exp(x₁ * 0.0024216)))...\n",
      "                                      )) + sin(x₁ * -0.079539)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 2.720e+03\n",
      "Progress: 123 / 1240 total iterations (9.919%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           2.170e+00  0.000e+00  y = 1.1143\n",
      "3           1.649e+00  1.374e-01  y = x₂ * 0.019943\n",
      "5           1.439e+00  6.787e-02  y = (x₂ * 0.015396) + 0.54856\n",
      "8           1.359e+00  1.910e-02  y = cos((x₀ * -0.22304) + 3.7941) + 1.3474\n",
      "17          1.345e+00  1.191e-03  y = cos(sin((x₀ * 0.13406) + inv(cos(exp(x₁ * 0.0024216)))...\n",
      "                                      )) + sin(x₁ * -0.079539)\n",
      "18          1.340e+00  3.159e-03  y = sin(x₁ * -0.079642) + (cos(sin((x₀ * 0.13578) + exp(in...\n",
      "                                      v(x₁) * 0.19049))) + 0.32143)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 2.960e+03\n",
      "Progress: 142 / 1240 total iterations (11.452%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           2.170e+00  0.000e+00  y = 1.1143\n",
      "3           1.649e+00  1.374e-01  y = x₂ * 0.019943\n",
      "5           1.439e+00  6.787e-02  y = (x₂ * 0.015396) + 0.54856\n",
      "7           1.318e+00  4.391e-02  y = ((x₂ * 0.00088262) + 0.048687) * x₀\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 2.990e+03\n",
      "Progress: 156 / 1240 total iterations (12.581%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           2.170e+00  0.000e+00  y = 1.1143\n",
      "3           1.649e+00  1.374e-01  y = x₂ * 0.019943\n",
      "5           1.439e+00  6.787e-02  y = (x₂ * 0.015396) + 0.54856\n",
      "7           1.318e+00  4.391e-02  y = ((x₂ * 0.00088262) + 0.048687) * x₀\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 2.540e+03\n",
      "Progress: 163 / 1240 total iterations (13.145%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           2.170e+00  0.000e+00  y = 1.1143\n",
      "3           1.649e+00  1.374e-01  y = x₂ * 0.019943\n",
      "5           1.439e+00  6.787e-02  y = (x₂ * 0.015396) + 0.54856\n",
      "7           1.318e+00  4.391e-02  y = ((x₂ * 0.00088262) + 0.048687) * x₀\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 1.920e+03\n",
      "Progress: 166 / 1240 total iterations (13.387%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           2.170e+00  0.000e+00  y = 1.1143\n",
      "3           1.649e+00  1.374e-01  y = x₂ * 0.019943\n",
      "5           1.439e+00  6.787e-02  y = (x₂ * 0.015396) + 0.54856\n",
      "7           1.318e+00  4.391e-02  y = ((x₂ * 0.00088262) + 0.048687) * x₀\n",
      "8           1.315e+00  2.423e-03  y = sin((x₂ * 0.00088262) + 0.048687) * x₀\n",
      "9           1.313e+00  1.858e-03  y = x₀ * sin(sin(0.048687 + (0.00088262 * x₂)))\n",
      "10          1.292e+00  1.608e-02  y = (x₀ * sin((0.00088262 * x₂) + 0.048687)) + 0.12628\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 1.510e+03\n",
      "Progress: 174 / 1240 total iterations (14.032%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           2.170e+00  0.000e+00  y = 1.1143\n",
      "3           1.649e+00  1.374e-01  y = x₂ * 0.019943\n",
      "5           1.439e+00  6.787e-02  y = (x₂ * 0.015396) + 0.54856\n",
      "7           1.318e+00  4.391e-02  y = ((x₂ * 0.00088262) + 0.048687) * x₀\n",
      "8           1.315e+00  2.423e-03  y = sin((x₂ * 0.00088262) + 0.048687) * x₀\n",
      "9           1.305e+00  7.750e-03  y = ((x₁ + (x₂ + x₂)) * 0.0076774) + -1.5551\n",
      "10          1.292e+00  1.019e-02  y = (x₀ * sin((0.00088262 * x₂) + 0.048687)) + 0.12628\n",
      "13          1.160e+00  3.574e-02  y = (((x₁ + x₂) * -0.98864) + x₂) + ((x₁ * 1.0151) + -6.56...\n",
      "                                      34)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 1.390e+03\n",
      "Progress: 185 / 1240 total iterations (14.919%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           2.170e+00  0.000e+00  y = 1.1143\n",
      "3           1.649e+00  1.374e-01  y = x₂ * 0.019943\n",
      "5           1.439e+00  6.787e-02  y = (x₂ * 0.015396) + 0.54856\n",
      "7           1.318e+00  4.391e-02  y = ((x₂ * 0.00088262) + 0.048687) * x₀\n",
      "8           1.315e+00  2.423e-03  y = sin((x₂ * 0.00088262) + 0.048687) * x₀\n",
      "9           1.305e+00  7.750e-03  y = ((x₁ + (x₂ + x₂)) * 0.0076774) + -1.5551\n",
      "10          1.292e+00  1.019e-02  y = (x₀ * sin((0.00088262 * x₂) + 0.048687)) + 0.12628\n",
      "13          1.160e+00  3.574e-02  y = (((x₁ + x₂) * -0.98864) + x₂) + ((x₁ * 1.0151) + -6.56...\n",
      "                                      34)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 1.520e+03\n",
      "Progress: 200 / 1240 total iterations (16.129%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           2.170e+00  0.000e+00  y = 1.1143\n",
      "3           1.649e+00  1.374e-01  y = x₂ * 0.019943\n",
      "5           1.439e+00  6.787e-02  y = (x₂ * 0.015396) + 0.54856\n",
      "7           1.318e+00  4.391e-02  y = ((x₂ * 0.00088262) + 0.048687) * x₀\n",
      "8           1.315e+00  2.423e-03  y = sin((x₂ * 0.00088262) + 0.048687) * x₀\n",
      "9           1.305e+00  7.750e-03  y = ((x₁ + (x₂ + x₂)) * 0.0076774) + -1.5551\n",
      "10          1.292e+00  1.019e-02  y = (x₀ * sin((0.00088262 * x₂) + 0.048687)) + 0.12628\n",
      "13          1.160e+00  3.574e-02  y = (((x₁ + x₂) * -0.98864) + x₂) + ((x₁ * 1.0151) + -6.56...\n",
      "                                      34)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n",
      "\n",
      "Expressions evaluated per second: 1.790e+03\n",
      "Progress: 212 / 1240 total iterations (17.097%)\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Complexity  Loss       Score      Equation\n",
      "1           2.170e+00  0.000e+00  y = 1.1143\n",
      "3           1.649e+00  1.374e-01  y = x₂ * 0.019943\n",
      "5           1.439e+00  6.787e-02  y = (x₂ * 0.015396) + 0.54856\n",
      "7           1.318e+00  4.391e-02  y = ((x₂ * 0.00088262) + 0.048687) * x₀\n",
      "8           1.315e+00  2.423e-03  y = sin((x₂ * 0.00088262) + 0.048687) * x₀\n",
      "9           1.305e+00  7.750e-03  y = ((x₁ + (x₂ + x₂)) * 0.0076774) + -1.5551\n",
      "10          1.292e+00  1.019e-02  y = (x₀ * sin((0.00088262 * x₂) + 0.048687)) + 0.12628\n",
      "13          1.160e+00  3.574e-02  y = (((x₁ + x₂) * -0.98864) + x₂) + ((x₁ * 1.0151) + -6.56...\n",
      "                                      34)\n",
      "───────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Press 'q' and then <enter> to stop execution early.\n"
     ]
    }
   ],
   "source": [
    "for X, y, mask, meta in loader:\n",
    "    X = X.squeeze(0).numpy()        # (3, H, W)\n",
    "    y = y.squeeze(0).numpy()        # (1, H, W)\n",
    "    mask = mask.squeeze(0).numpy()  # (1, H, W)\n",
    "\n",
    "    # Flatten\n",
    "    X_flat = X.reshape(3, -1).T      # (n_pixels, 3)\n",
    "    y_flat = y.reshape(-1)           # (n_pixels,)\n",
    "    mask_flat = mask.reshape(-1)     # (n_pixels,)\n",
    "\n",
    "    # Keep only valid pixels\n",
    "    X_flat = X_flat[mask_flat]\n",
    "    y_flat = y_flat[mask_flat]\n",
    "\n",
    "    # 🔹 Random subsample (e.g. 20,000 samples max)\n",
    "    n_samples = min(20000, len(y_flat))\n",
    "    idx = np.random.choice(len(y_flat), n_samples, replace=False)\n",
    "    X_flat = X_flat[idx]\n",
    "    y_flat = y_flat[idx]\n",
    "\n",
    "    # Train/val split\n",
    "    split = int(0.8 * n_samples)\n",
    "    X_train, X_val = X_flat[:split], X_flat[split:]\n",
    "    y_train, y_val = y_flat[:split], y_flat[split:]\n",
    "\n",
    "    # Fit PySR\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    print(\"PySR finished. Discovered equations:\")\n",
    "    print(model)\n",
    "\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    \n",
    "    loss = masked_mse_loss(y_pred_val, y_val, mask_flat)\n",
    "    print(f\"year={meta['year'][0].item()}, sample={meta['sample'][0].item()}, loss={float(loss):.4f}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10acd961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a27b579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from pysr import PySRRegressor\n",
    "\n",
    "# ----------------------\n",
    "# 1. Load data\n",
    "# ----------------------\n",
    "variables = {\n",
    "    \"lai\": ds_lai[\"LAI\"],\n",
    "    \"ssrd\": ds_ssrd[\"SSRD\"],\n",
    "    \"t2m\": ds_t2m[\"T2M\"],\n",
    "    \"tp\": ds_tp[\"TP\"],\n",
    "}\n",
    "\n",
    "# Stack spatial/temporal dimensions into samples\n",
    "X = np.column_stack([\n",
    "    variables[\"ssrd\"].values.ravel(),\n",
    "    variables[\"t2m\"].values.ravel(),\n",
    "    variables[\"tp\"].values.ravel(),\n",
    "])\n",
    "y = variables[\"lai\"].values.ravel()\n",
    "\n",
    "# Remove NaNs\n",
    "mask = ~np.isnan(X).any(axis=1) & ~np.isnan(y)\n",
    "X, y = X[mask], y[mask]\n",
    "\n",
    "# ----------------------\n",
    "# 2. Downsample (optional)\n",
    "# ----------------------\n",
    "# Randomly select a subset (say 10k samples) for speed\n",
    "n_samples = min(10000, len(y))\n",
    "idx = np.random.choice(len(y), size=n_samples, replace=False)\n",
    "X, y = X[idx], y[idx]\n",
    "\n",
    "# ----------------------\n",
    "# 3. Train/Validation split\n",
    "# ----------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# ----------------------\n",
    "# 4. Define and fit PySR model\n",
    "# ----------------------\n",
    "model = PySRRegressor(\n",
    "    niterations=100,  # increase later\n",
    "    binary_operators=[\"+\", \"-\", \"*\", \"/\"],\n",
    "    unary_operators=[\"log\", \"exp\", \"sin\", \"cos\"],\n",
    "    model_selection=\"best\",  # pick best equation\n",
    "    progress=True,\n",
    "    verbosity=1,\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# ----------------------\n",
    "# 5. Evaluate\n",
    "# ----------------------\n",
    "y_val_pred = model.predict(X_val)\n",
    "\n",
    "r2 = r2_score(y_val, y_val_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "\n",
    "print(\"\\nBest equation found:\")\n",
    "print(model.get_best())\n",
    "\n",
    "print(f\"\\nValidation R²: {r2:.3f}, RMSE: {rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae85db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #!/usr/bin/env python3\n",
    "# \"\"\"\n",
    "# pysr_lai.py\n",
    "\n",
    "# Small PySR symbolic regression workflow to predict LAI from ssrd, t2m, tp.\n",
    "# Usage:\n",
    "#     python pysr_lai.py --lai /path/to/lai.nc --ssrd /path/to/ssrd.nc --t2m /path/to/t2m.nc --tp /path/to/tp.nc\n",
    "# \"\"\"\n",
    "\n",
    "# import argparse\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import xarray as xr\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from pysr import PySRRegressor  # make sure pysr is installed\n",
    "\n",
    "# def open_primary_var(path):\n",
    "#     ds = xr.open_dataset(path)\n",
    "#     # If dataset contains exactly 1 data variable, return it; otherwise pick the first\n",
    "#     data_vars = list(ds.data_vars)\n",
    "#     if len(data_vars) == 0:\n",
    "#         raise ValueError(f\"No data variables found in {path}\")\n",
    "#     if len(data_vars) > 1:\n",
    "#         print(f\"Warning: {path} contains multiple data variables. Using '{data_vars[0]}' by default.\")\n",
    "#     var = data_vars[0]\n",
    "#     return ds[var].load()  # load into memory (careful with size) - we coarsen later if needed\n",
    "\n",
    "# def coarsen_all(arrays_dict, lat_step=10, lon_step=10, time_slice=None):\n",
    "#     \"\"\"Coarsen all arrays consistently by slicing every lat_step/lon_step and optionally restricting time.\"\"\"\n",
    "#     out = {}\n",
    "#     for k, da in arrays_dict.items():\n",
    "#         # ensure coordinates named 'latitude' / 'longitude' or accept variants\n",
    "#         # We'll use index-based slicing to be robust\n",
    "#         lat_slice = slice(None, None, lat_step)\n",
    "#         lon_slice = slice(None, None, lon_step)\n",
    "#         if time_slice is not None:\n",
    "#             dsliced = da.isel(time=time_slice, latitude=lat_slice, longitude=lon_slice)\n",
    "#         else:\n",
    "#             dsliced = da.isel(latitude=lat_slice, longitude=lon_slice)\n",
    "#         out[k] = dsliced\n",
    "#     return out\n",
    "\n",
    "# def stack_to_samples(da):\n",
    "#     \"\"\"Stack (time, latitude, longitude) into one axis named 'sample' and return values and coords.\"\"\"\n",
    "#     stacked = da.stack(sample=(\"time\", da.dims[-2], da.dims[-1]))\n",
    "#     return stacked.values, stacked\n",
    "\n",
    "# def main(args):\n",
    "#     # 1. Open datasets (auto variable selection)\n",
    "#     print(\"Opening datasets...\")\n",
    "#     lai_da = open_primary_var(args.lai)\n",
    "#     ssrd_da = open_primary_var(args.ssrd)\n",
    "#     t2m_da = open_primary_var(args.t2m)\n",
    "#     tp_da = open_primary_var(args.tp)\n",
    "\n",
    "#     print(\"LAI variable dims:\", lai_da.dims, \"shape:\", lai_da.shape)\n",
    "#     # 2. Coarsen / subsample to keep memory reasonable\n",
    "#     print(\"Coarsening with lat_step=\", args.lat_step, \" lon_step=\", args.lon_step)\n",
    "#     arrays = {\"lai\": lai_da, \"ssrd\": ssrd_da, \"t2m\": t2m_da, \"tp\": tp_da}\n",
    "#     arrays = coarsen_all(arrays, lat_step=args.lat_step, lon_step=args.lon_step, time_slice=args.time_slice)\n",
    "\n",
    "#     # 3. Optionally further restrict time range\n",
    "#     # (time_slice taken in coarsen_all above)\n",
    "\n",
    "#     # 4. Stack to (samples,)\n",
    "#     print(\"Stacking arrays to samples...\")\n",
    "#     y_vals, y_stack = stack_to_samples(arrays[\"lai\"])\n",
    "#     X_list = []\n",
    "#     names = []\n",
    "#     for name in [\"ssrd\", \"t2m\", \"tp\"]:\n",
    "#         vals, _ = stack_to_samples(arrays[name])\n",
    "#         X_list.append(vals)\n",
    "#         names.append(name)\n",
    "#     X = np.vstack(X_list).T  # shape (n_samples, 3)\n",
    "#     print(\"X shape:\", X.shape, \"y shape:\", y_vals.shape)\n",
    "\n",
    "#     # 5. Clean NaNs\n",
    "#     print(\"Cleaning NaNs...\")\n",
    "#     mask = ~np.isnan(X).any(axis=1) & ~np.isnan(y_vals)\n",
    "#     print(f\"Samples before: {X.shape[0]}, after removing NaNs: {mask.sum()}\")\n",
    "#     X_clean = X[mask]\n",
    "#     y_clean = y_vals[mask]\n",
    "\n",
    "#     # 6. Optionally subsample for speed\n",
    "#     if args.max_samples and mask.sum() > args.max_samples:\n",
    "#         rng = np.random.default_rng(args.random_seed)\n",
    "#         idx = rng.choice(np.arange(mask.sum()), size=args.max_samples, replace=False)\n",
    "#         X_clean = X_clean[idx]\n",
    "#         y_clean = y_clean[idx]\n",
    "#         print(f\"Subsampled to {args.max_samples} samples for PySR speed.\")\n",
    "\n",
    "#     # 7. Standardize features (helps symbolic regression numeric stability)\n",
    "#     scaler = StandardScaler()\n",
    "#     X_scaled = scaler.fit_transform(X_clean)\n",
    "\n",
    "#     # 8. Train/test split\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_clean, test_size=args.test_size, random_state=args.random_seed)\n",
    "#     print(\"Train/test sizes:\", X_train.shape[0], X_test.shape[0])\n",
    "\n",
    "#     # 9. Run PySR (small config by default; tune niterations for better results)\n",
    "#     print(\"Running PySR symbolic regression...\")\n",
    "#     model = PySRRegressor(\n",
    "#         niterations=args.niterations,\n",
    "#         binary_operators=[\"+\", \"-\", \"*\", \"/\"],\n",
    "#         unary_operators=[\"sin\", \"cos\", \"exp\", \"log\", \"sqrt\"],\n",
    "#         populationsize=args.population_size,\n",
    "#         maxsize=args.maxsize,\n",
    "#         ncyclesperiteration=60,\n",
    "#         loss=\"loss01\",  # default: mean absolute error-like; you can change\n",
    "#         model_selection=\"best\",\n",
    "#         timeout=args.timeout,  # seconds (None -> no timeout)\n",
    "#         tempdir=args.output_dir,\n",
    "#         multithreading=args.n_jobs,\n",
    "#         verbosity=1,\n",
    "#         progress=args.show_progress\n",
    "#     )\n",
    "\n",
    "#     # Provide feature names for clearer equations\n",
    "#     feature_names = names\n",
    "#     model.fit(X_train, y_train, feature_names=feature_names)\n",
    "\n",
    "#     print(\"PySR finished. Equations:\")\n",
    "#     print(model)\n",
    "\n",
    "#     # 10. Evaluate on test set\n",
    "#     y_pred_test = model.predict(X_test)\n",
    "#     r2_test = np.corrcoef(y_test, y_pred_test)[0,1]**2  # approx R^2\n",
    "#     print(f\"Approx R^2 on test set (corr-based): {r2_test:.4f}\")\n",
    "\n",
    "#     # 11. Save best equations table\n",
    "#     eqs_df = model.equations_\n",
    "#     eqs_path = os.path.join(args.output_dir, \"pysr_equations.csv\")\n",
    "#     eqs_df.to_csv(eqs_path, index=False)\n",
    "#     print(\"Saved equations to:\", eqs_path)\n",
    "\n",
    "#     # 12. Create predictions for the full stacked sample set (where mask true)\n",
    "#     print(\"Predicting full grid (where data available)...\")\n",
    "#     # Scale all non-NaN X to feed into model:\n",
    "#     X_all = X[mask]  # corresponds to y_clean rows\n",
    "#     X_all_scaled = scaler.transform(X_all if len(X_all.shape) == 2 else X_all.reshape(-1, X_all.shape[-1]))\n",
    "#     y_all_pred = model.predict(X_all_scaled)\n",
    "\n",
    "#     # Reconstruct y_pred into full sample-shaped array (fill with NaNs where mask was False)\n",
    "#     y_full = np.full(y_vals.shape, np.nan, dtype=float)\n",
    "#     y_full[mask] = y_all_pred\n",
    "\n",
    "#     # Put back into DataArray with original stacked coords, then unstack back to (time, lat, lon)\n",
    "#     pred_da = xr.DataArray(y_full, coords=[y_stack.sample], dims=[\"sample\"])\n",
    "#     pred_da = pred_da.unstack(\"sample\")\n",
    "#     pred_da.name = \"lai_pred\"\n",
    "#     pred_da.attrs[\"note\"] = \"Predicted LAI from PySR symbolic regression\"\n",
    "\n",
    "#     # Save predicted netcdf\n",
    "#     pred_nc_path = os.path.join(args.output_dir, \"lai_pred_pysr.nc\")\n",
    "#     pred_da.to_dataset().to_netcdf(pred_nc_path)\n",
    "#     print(\"Saved prediction netCDF to:\", pred_nc_path)\n",
    "\n",
    "#     # 13. Quick diagnostic scatter plot test vs observed (random subset)\n",
    "#     plt.figure(figsize=(6,6))\n",
    "#     rng = np.random.default_rng(args.random_seed)\n",
    "#     nplot = min(5000, len(y_test))\n",
    "#     sel = rng.choice(len(y_test), size=nplot, replace=False)\n",
    "#     plt.scatter(y_test[sel], y_pred_test[sel], s=2, alpha=0.6)\n",
    "#     plt.xlabel(\"observed LAI (test)\")\n",
    "#     plt.ylabel(\"predicted LAI\")\n",
    "#     plt.title(\"PySR LAI: observed vs predicted (test set)\")\n",
    "#     plt.grid(True)\n",
    "#     scatter_path = os.path.join(args.output_dir, \"obs_vs_pred_test.png\")\n",
    "#     plt.savefig(scatter_path, dpi=150)\n",
    "#     plt.close()\n",
    "#     print(\"Saved scatter plot to:\", scatter_path)\n",
    "\n",
    "#     print(\"Done. Check output folder:\", args.output_dir)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     p = argparse.ArgumentParser(description=\"Small PySR workflow to predict LAI from ssrd, t2m, tp\")\n",
    "#     p.add_argument(\"--lai\", required=True, help=\"Path to LAI netcdf\")\n",
    "#     p.add_argument(\"--ssrd\", required=True, help=\"Path to SSRD netcdf\")\n",
    "#     p.add_argument(\"--t2m\", required=True, help=\"Path to T2M netcdf\")\n",
    "#     p.add_argument(\"--tp\", required=True, help=\"Path to TP netcdf\")\n",
    "#     p.add_argument(\"--lat-step\", type=int, default=10, help=\"Spatial subsampling step for latitude (default 10)\")\n",
    "#     p.add_argument(\"--lon-step\", type=int, default=10, help=\"Spatial subsampling step for longitude (default 10)\")\n",
    "#     p.add_argument(\"--time-slice\", type=int, nargs='+', default=None,\n",
    "#                    help=\"Optional time slice indices for .isel(time=...) e.g. --time-slice 0 23 (use first 24 timesteps).\")\n",
    "#     p.add_argument(\"--max-samples\", type=int, default=200000, help=\"Max number of samples to keep for PySR (subsamples randomly).\")\n",
    "#     p.add_argument(\"--niterations\", type=int, default=40, help=\"PySR niterations (increase for better results).\")\n",
    "#     p.add_argument(\"--population-size\", type=int, dest=\"population_size\", default=100, help=\"PySR population size.\")\n",
    "#     p.add_argument(\"--maxsize\", type=int, default=20, help=\"Max expression size for PySR.\")\n",
    "#     p.add_argument(\"--timeout\", type=int, default=None, help=\"Timeout in seconds for PySR (optional).\")\n",
    "#     p.add_argument(\"--n-jobs\", type=int, default=4, dest=\"n_jobs\", help=\"Number of threads for PySR.\")\n",
    "#     p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set proportion.\")\n",
    "#     p.add_argument(\"--random-seed\", type=int, default=0, help=\"Random seed.\")\n",
    "#     p.add_argument(\"--output-dir\", default=\"pysr_output\", help=\"Directory to store outputs.\")\n",
    "#     p.add_argument(\"--show-progress\", action=\"store_true\", help=\"Show PySR progress if supported.\")\n",
    "#     args = p.parse_args()\n",
    "\n",
    "#     # normalize time_slice format for isel usage\n",
    "#     if args.time_slice is not None:\n",
    "#         # if user provided two ints like \"0 23\", we make slice(0,24)\n",
    "#         if len(args.time_slice) == 2:\n",
    "#             start, stop = args.time_slice\n",
    "#             args.time_slice = slice(start, stop + 1)\n",
    "#         else:\n",
    "#             # if list of indices, keep as list\n",
    "#             args.time_slice = args.time_slice\n",
    "\n",
    "#     os.makedirs(args.output_dir, exist_ok=True)\n",
    "#     main(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
