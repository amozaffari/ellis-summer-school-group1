{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2394d95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected IPython. Loading juliacall extension. See https://juliapy.github.io/PythonCall.jl/stable/compat/#IPython\n"
     ]
    }
   ],
   "source": [
    "from pysr import PySRRegressor\n",
    "\n",
    "model = PySRRegressor(\n",
    "    maxsize=20,\n",
    "    niterations=40,  # < Increase me for better results\n",
    "    binary_operators=[\"+\", \"*\"],\n",
    "    unary_operators=[\n",
    "        \"cos\",\n",
    "        \"exp\",\n",
    "        \"sin\",\n",
    "        \"inv(x) = 1/x\",\n",
    "        # ^ Custom operator (julia syntax)\n",
    "    ],\n",
    "    extra_sympy_mappings={\"inv\": lambda x: 1 / x},\n",
    "    # ^ Define operator for SymPy as well\n",
    "    elementwise_loss=\"loss(prediction, target) = (prediction - target)^2\",\n",
    "    # ^ Custom loss function (julia syntax)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70deefc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = 2 * np.random.randn(100, 5)\n",
    "y = 2.5382 * np.cos(X[:, 3]) + X[:, 0] ** 2 - 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2c61fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/u/mp186/conda-envs/lai_env/lib/python3.12/site-packages/pysr/sr.py:2811: UserWarning: Note: it looks like you are running in Jupyter. The progress bar will be turned off.\n",
      "  warnings.warn(\n",
      "Compiling Julia backend...\n"
     ]
    }
   ],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f639037f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39059ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec021a61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10acd961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae85db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "pysr_lai.py\n",
    "\n",
    "Small PySR symbolic regression workflow to predict LAI from ssrd, t2m, tp.\n",
    "Usage:\n",
    "    python pysr_lai.py --lai /path/to/lai.nc --ssrd /path/to/ssrd.nc --t2m /path/to/t2m.nc --tp /path/to/tp.nc\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pysr import PySRRegressor  # make sure pysr is installed\n",
    "\n",
    "def open_primary_var(path):\n",
    "    ds = xr.open_dataset(path)\n",
    "    # If dataset contains exactly 1 data variable, return it; otherwise pick the first\n",
    "    data_vars = list(ds.data_vars)\n",
    "    if len(data_vars) == 0:\n",
    "        raise ValueError(f\"No data variables found in {path}\")\n",
    "    if len(data_vars) > 1:\n",
    "        print(f\"Warning: {path} contains multiple data variables. Using '{data_vars[0]}' by default.\")\n",
    "    var = data_vars[0]\n",
    "    return ds[var].load()  # load into memory (careful with size) - we coarsen later if needed\n",
    "\n",
    "def coarsen_all(arrays_dict, lat_step=10, lon_step=10, time_slice=None):\n",
    "    \"\"\"Coarsen all arrays consistently by slicing every lat_step/lon_step and optionally restricting time.\"\"\"\n",
    "    out = {}\n",
    "    for k, da in arrays_dict.items():\n",
    "        # ensure coordinates named 'latitude' / 'longitude' or accept variants\n",
    "        # We'll use index-based slicing to be robust\n",
    "        lat_slice = slice(None, None, lat_step)\n",
    "        lon_slice = slice(None, None, lon_step)\n",
    "        if time_slice is not None:\n",
    "            dsliced = da.isel(time=time_slice, latitude=lat_slice, longitude=lon_slice)\n",
    "        else:\n",
    "            dsliced = da.isel(latitude=lat_slice, longitude=lon_slice)\n",
    "        out[k] = dsliced\n",
    "    return out\n",
    "\n",
    "def stack_to_samples(da):\n",
    "    \"\"\"Stack (time, latitude, longitude) into one axis named 'sample' and return values and coords.\"\"\"\n",
    "    stacked = da.stack(sample=(\"time\", da.dims[-2], da.dims[-1]))\n",
    "    return stacked.values, stacked\n",
    "\n",
    "def main(args):\n",
    "    # 1. Open datasets (auto variable selection)\n",
    "    print(\"Opening datasets...\")\n",
    "    lai_da = open_primary_var(args.lai)\n",
    "    ssrd_da = open_primary_var(args.ssrd)\n",
    "    t2m_da = open_primary_var(args.t2m)\n",
    "    tp_da = open_primary_var(args.tp)\n",
    "\n",
    "    print(\"LAI variable dims:\", lai_da.dims, \"shape:\", lai_da.shape)\n",
    "    # 2. Coarsen / subsample to keep memory reasonable\n",
    "    print(\"Coarsening with lat_step=\", args.lat_step, \" lon_step=\", args.lon_step)\n",
    "    arrays = {\"lai\": lai_da, \"ssrd\": ssrd_da, \"t2m\": t2m_da, \"tp\": tp_da}\n",
    "    arrays = coarsen_all(arrays, lat_step=args.lat_step, lon_step=args.lon_step, time_slice=args.time_slice)\n",
    "\n",
    "    # 3. Optionally further restrict time range\n",
    "    # (time_slice taken in coarsen_all above)\n",
    "\n",
    "    # 4. Stack to (samples,)\n",
    "    print(\"Stacking arrays to samples...\")\n",
    "    y_vals, y_stack = stack_to_samples(arrays[\"lai\"])\n",
    "    X_list = []\n",
    "    names = []\n",
    "    for name in [\"ssrd\", \"t2m\", \"tp\"]:\n",
    "        vals, _ = stack_to_samples(arrays[name])\n",
    "        X_list.append(vals)\n",
    "        names.append(name)\n",
    "    X = np.vstack(X_list).T  # shape (n_samples, 3)\n",
    "    print(\"X shape:\", X.shape, \"y shape:\", y_vals.shape)\n",
    "\n",
    "    # 5. Clean NaNs\n",
    "    print(\"Cleaning NaNs...\")\n",
    "    mask = ~np.isnan(X).any(axis=1) & ~np.isnan(y_vals)\n",
    "    print(f\"Samples before: {X.shape[0]}, after removing NaNs: {mask.sum()}\")\n",
    "    X_clean = X[mask]\n",
    "    y_clean = y_vals[mask]\n",
    "\n",
    "    # 6. Optionally subsample for speed\n",
    "    if args.max_samples and mask.sum() > args.max_samples:\n",
    "        rng = np.random.default_rng(args.random_seed)\n",
    "        idx = rng.choice(np.arange(mask.sum()), size=args.max_samples, replace=False)\n",
    "        X_clean = X_clean[idx]\n",
    "        y_clean = y_clean[idx]\n",
    "        print(f\"Subsampled to {args.max_samples} samples for PySR speed.\")\n",
    "\n",
    "    # 7. Standardize features (helps symbolic regression numeric stability)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_clean)\n",
    "\n",
    "    # 8. Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_clean, test_size=args.test_size, random_state=args.random_seed)\n",
    "    print(\"Train/test sizes:\", X_train.shape[0], X_test.shape[0])\n",
    "\n",
    "    # 9. Run PySR (small config by default; tune niterations for better results)\n",
    "    print(\"Running PySR symbolic regression...\")\n",
    "    model = PySRRegressor(\n",
    "        niterations=args.niterations,\n",
    "        binary_operators=[\"+\", \"-\", \"*\", \"/\"],\n",
    "        unary_operators=[\"sin\", \"cos\", \"exp\", \"log\", \"sqrt\"],\n",
    "        populationsize=args.population_size,\n",
    "        maxsize=args.maxsize,\n",
    "        ncyclesperiteration=60,\n",
    "        loss=\"loss01\",  # default: mean absolute error-like; you can change\n",
    "        model_selection=\"best\",\n",
    "        timeout=args.timeout,  # seconds (None -> no timeout)\n",
    "        tempdir=args.output_dir,\n",
    "        multithreading=args.n_jobs,\n",
    "        verbosity=1,\n",
    "        progress=args.show_progress\n",
    "    )\n",
    "\n",
    "    # Provide feature names for clearer equations\n",
    "    feature_names = names\n",
    "    model.fit(X_train, y_train, feature_names=feature_names)\n",
    "\n",
    "    print(\"PySR finished. Equations:\")\n",
    "    print(model)\n",
    "\n",
    "    # 10. Evaluate on test set\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    r2_test = np.corrcoef(y_test, y_pred_test)[0,1]**2  # approx R^2\n",
    "    print(f\"Approx R^2 on test set (corr-based): {r2_test:.4f}\")\n",
    "\n",
    "    # 11. Save best equations table\n",
    "    eqs_df = model.equations_\n",
    "    eqs_path = os.path.join(args.output_dir, \"pysr_equations.csv\")\n",
    "    eqs_df.to_csv(eqs_path, index=False)\n",
    "    print(\"Saved equations to:\", eqs_path)\n",
    "\n",
    "    # 12. Create predictions for the full stacked sample set (where mask true)\n",
    "    print(\"Predicting full grid (where data available)...\")\n",
    "    # Scale all non-NaN X to feed into model:\n",
    "    X_all = X[mask]  # corresponds to y_clean rows\n",
    "    X_all_scaled = scaler.transform(X_all if len(X_all.shape) == 2 else X_all.reshape(-1, X_all.shape[-1]))\n",
    "    y_all_pred = model.predict(X_all_scaled)\n",
    "\n",
    "    # Reconstruct y_pred into full sample-shaped array (fill with NaNs where mask was False)\n",
    "    y_full = np.full(y_vals.shape, np.nan, dtype=float)\n",
    "    y_full[mask] = y_all_pred\n",
    "\n",
    "    # Put back into DataArray with original stacked coords, then unstack back to (time, lat, lon)\n",
    "    pred_da = xr.DataArray(y_full, coords=[y_stack.sample], dims=[\"sample\"])\n",
    "    pred_da = pred_da.unstack(\"sample\")\n",
    "    pred_da.name = \"lai_pred\"\n",
    "    pred_da.attrs[\"note\"] = \"Predicted LAI from PySR symbolic regression\"\n",
    "\n",
    "    # Save predicted netcdf\n",
    "    pred_nc_path = os.path.join(args.output_dir, \"lai_pred_pysr.nc\")\n",
    "    pred_da.to_dataset().to_netcdf(pred_nc_path)\n",
    "    print(\"Saved prediction netCDF to:\", pred_nc_path)\n",
    "\n",
    "    # 13. Quick diagnostic scatter plot test vs observed (random subset)\n",
    "    plt.figure(figsize=(6,6))\n",
    "    rng = np.random.default_rng(args.random_seed)\n",
    "    nplot = min(5000, len(y_test))\n",
    "    sel = rng.choice(len(y_test), size=nplot, replace=False)\n",
    "    plt.scatter(y_test[sel], y_pred_test[sel], s=2, alpha=0.6)\n",
    "    plt.xlabel(\"observed LAI (test)\")\n",
    "    plt.ylabel(\"predicted LAI\")\n",
    "    plt.title(\"PySR LAI: observed vs predicted (test set)\")\n",
    "    plt.grid(True)\n",
    "    scatter_path = os.path.join(args.output_dir, \"obs_vs_pred_test.png\")\n",
    "    plt.savefig(scatter_path, dpi=150)\n",
    "    plt.close()\n",
    "    print(\"Saved scatter plot to:\", scatter_path)\n",
    "\n",
    "    print(\"Done. Check output folder:\", args.output_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    p = argparse.ArgumentParser(description=\"Small PySR workflow to predict LAI from ssrd, t2m, tp\")\n",
    "    p.add_argument(\"--lai\", required=True, help=\"Path to LAI netcdf\")\n",
    "    p.add_argument(\"--ssrd\", required=True, help=\"Path to SSRD netcdf\")\n",
    "    p.add_argument(\"--t2m\", required=True, help=\"Path to T2M netcdf\")\n",
    "    p.add_argument(\"--tp\", required=True, help=\"Path to TP netcdf\")\n",
    "    p.add_argument(\"--lat-step\", type=int, default=10, help=\"Spatial subsampling step for latitude (default 10)\")\n",
    "    p.add_argument(\"--lon-step\", type=int, default=10, help=\"Spatial subsampling step for longitude (default 10)\")\n",
    "    p.add_argument(\"--time-slice\", type=int, nargs='+', default=None,\n",
    "                   help=\"Optional time slice indices for .isel(time=...) e.g. --time-slice 0 23 (use first 24 timesteps).\")\n",
    "    p.add_argument(\"--max-samples\", type=int, default=200000, help=\"Max number of samples to keep for PySR (subsamples randomly).\")\n",
    "    p.add_argument(\"--niterations\", type=int, default=40, help=\"PySR niterations (increase for better results).\")\n",
    "    p.add_argument(\"--population-size\", type=int, dest=\"population_size\", default=100, help=\"PySR population size.\")\n",
    "    p.add_argument(\"--maxsize\", type=int, default=20, help=\"Max expression size for PySR.\")\n",
    "    p.add_argument(\"--timeout\", type=int, default=None, help=\"Timeout in seconds for PySR (optional).\")\n",
    "    p.add_argument(\"--n-jobs\", type=int, default=4, dest=\"n_jobs\", help=\"Number of threads for PySR.\")\n",
    "    p.add_argument(\"--test-size\", type=float, default=0.2, help=\"Test set proportion.\")\n",
    "    p.add_argument(\"--random-seed\", type=int, default=0, help=\"Random seed.\")\n",
    "    p.add_argument(\"--output-dir\", default=\"pysr_output\", help=\"Directory to store outputs.\")\n",
    "    p.add_argument(\"--show-progress\", action=\"store_true\", help=\"Show PySR progress if supported.\")\n",
    "    args = p.parse_args()\n",
    "\n",
    "    # normalize time_slice format for isel usage\n",
    "    if args.time_slice is not None:\n",
    "        # if user provided two ints like \"0 23\", we make slice(0,24)\n",
    "        if len(args.time_slice) == 2:\n",
    "            start, stop = args.time_slice\n",
    "            args.time_slice = slice(start, stop + 1)\n",
    "        else:\n",
    "            # if list of indices, keep as list\n",
    "            args.time_slice = args.time_slice\n",
    "\n",
    "    os.makedirs(args.output_dir, exist_ok=True)\n",
    "    main(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
