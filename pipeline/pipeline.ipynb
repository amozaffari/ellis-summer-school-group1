{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a3cfda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_era5_lai_whole.py\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa592016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_era5_lai_whole.py\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ERA5LAIWholeWorld(Dataset):\n",
    "    \"\"\"\n",
    "    One YEAR, 24 items (one per 15-day sample). Each __getitem__ returns:\n",
    "        X: (3, H, W)  -> [ssrd, t2m, tp]  (raw/anom/z as chosen)\n",
    "        y: (1, H, W)  -> LAI (raw or anomaly)\n",
    "        mask: (1, H, W) boolean (True where y is valid)\n",
    "        meta: dict with 'year' and 'sample'\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        year: int,\n",
    "        era5_mode=\"anom\",   # \"raw\" | \"anom\" | \"z\"\n",
    "        lai_mode=\"raw\",     # \"raw\" | \"anom\"\n",
    "        paths=None,\n",
    "        engine=\"netcdf4\",\n",
    "        robust_nan=True,\n",
    "        sample_indices=None,  # default: all 0..23\n",
    "    ):\n",
    "        assert era5_mode in (\"raw\",\"anom\",\"z\")\n",
    "        assert lai_mode in (\"raw\",\"anom\")\n",
    "        self.year = int(year)\n",
    "        self.era5_mode = era5_mode\n",
    "        self.lai_mode = lai_mode\n",
    "        self.engine = engine\n",
    "        self.robust_nan = robust_nan\n",
    "        self.sample_indices = list(range(24)) if sample_indices is None else list(sample_indices)\n",
    "\n",
    "        # default paths (override with 'paths' dict)\n",
    "        default_paths = {\n",
    "            \"era5_root\": \"/ptmp/mp002/ellis/lai\",\n",
    "            \"era5_anom_dir\": \"/ptmp/mp040/outputdir/era5/anom\",   # your anomalies/z-scores\n",
    "            \"lai_root\": \"/ptmp/mp002/ellis/lai/lai\",\n",
    "            \"lai_tmpl\": \"LAI.1440.720.{year}.nc\",\n",
    "            \"lai_anom_dir\": \"/ptmp/mp002/ellis/lai/anom\",         # change if different\n",
    "        }\n",
    "        self.paths = default_paths if paths is None else {**default_paths, **paths}\n",
    "\n",
    "        # open all arrays for that year\n",
    "        self._open_year()\n",
    "\n",
    "    def _open_da(self, path, varname):\n",
    "        ds = xr.open_dataset(path, engine=self.engine)\n",
    "        da = ds[varname]\n",
    "        if self.robust_nan:\n",
    "            fv = da.attrs.get(\"_FillValue\", None)\n",
    "            if fv is not None:\n",
    "                da = da.where(da != fv)\n",
    "        # north-up\n",
    "        lat_name = \"lat\" if \"lat\" in da.coords else \"latitude\"\n",
    "        da = da.sortby(lat_name)\n",
    "        # attach sample coord\n",
    "        if \"time\" in da.dims and da.sizes[\"time\"] == 24:\n",
    "            da = da.assign_coords(sample=(\"time\", np.arange(24)))\n",
    "        return da\n",
    "\n",
    "    def _open_year(self):\n",
    "        y = self.year\n",
    "\n",
    "        # ERA5 inputs\n",
    "        if self.era5_mode == \"raw\":\n",
    "            f_ssrd = os.path.join(self.paths[\"era5_root\"], \"ssrd\", f\"ssrd.15daily.fc.era5.1440.720.{y}.nc\")\n",
    "            f_t2m  = os.path.join(self.paths[\"era5_root\"], \"t2m\",  f\"t2m.15daily.an.era5.1440.720.{y}.nc\")\n",
    "            f_tp   = os.path.join(self.paths[\"era5_root\"], \"tp\",   f\"tp.15daily.fc.era5.1440.720.{y}.nc\")\n",
    "            self.ssrd = self._open_da(f_ssrd, \"ssrd\")\n",
    "            self.t2m  = self._open_da(f_t2m,  \"t2m\")\n",
    "            self.tp   = self._open_da(f_tp,   \"tp\")\n",
    "        else:\n",
    "            suffix = \"anom\" if self.era5_mode == \"anom\" else \"z\"\n",
    "            base = self.paths[\"era5_anom_dir\"]\n",
    "            self.ssrd = self._open_da(os.path.join(base, f\"ssrd_{suffix}_{y}.nc\"), f\"ssrd_{suffix}\")\n",
    "            self.t2m  = self._open_da(os.path.join(base, f\"t2m_{suffix}_{y}.nc\"),  f\"t2m_{suffix}\")\n",
    "            self.tp   = self._open_da(os.path.join(base, f\"tp_{suffix}_{y}.nc\"),   f\"tp_{suffix}\")\n",
    "\n",
    "        # LAI target\n",
    "        lai_file = os.path.join(self.paths[\"lai_root\"], self.paths[\"lai_tmpl\"].format(year=y))\n",
    "        lai_var = self._infer_var(lai_file)\n",
    "        self.lai = self._open_da(lai_file, lai_var)\n",
    "\n",
    "        if self.lai_mode == \"anom\":\n",
    "            # use your saved LAI anomalies per year if available\n",
    "            lai_anom_dir = self.paths.get(\"lai_anom_dir\", self.paths[\"lai_root\"])\n",
    "            lai_anom_file = os.path.join(lai_anom_dir, f\"LAI_anom_{y}.nc\")\n",
    "            if os.path.exists(lai_anom_file):\n",
    "                self.lai = self._open_da(lai_anom_file, \"LAI_anom\")\n",
    "            else:\n",
    "                raise FileNotFoundError(f\"LAI anomaly file not found: {lai_anom_file}\")\n",
    "\n",
    "        # dims/coords\n",
    "        self.lat_name = \"lat\" if \"lat\" in self.lai.coords else \"latitude\"\n",
    "        self.lon_name = \"lon\" if \"lon\" in self.lai.coords else \"longitude\"\n",
    "\n",
    "        # sanity checks\n",
    "        for da, name in [(self.ssrd,\"ssrd\"),(self.t2m,\"t2m\"),(self.tp,\"tp\")]:\n",
    "            assert \"time\" in da.dims and da.sizes[\"time\"] == 24, f\"{name} expects 24 samples\"\n",
    "            assert da.sizes[self.lat_name] == self.lai.sizes[self.lat_name]\n",
    "            assert da.sizes[self.lon_name] == self.lai.sizes[self.lon_name]\n",
    "        assert \"time\" in self.lai.dims and self.lai.sizes[\"time\"] == 24\n",
    "\n",
    "    def _infer_var(self, nc_path):\n",
    "        with xr.open_dataset(nc_path, engine=self.engine) as ds:\n",
    "            for v in ds.data_vars:\n",
    "                if ds[v].ndim >= 2:\n",
    "                    return v\n",
    "        raise RuntimeError(f\"No data variable in {nc_path}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sample_indices)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        s = self.sample_indices[i]\n",
    "        # features (H,W) each\n",
    "        x_ssrd = self.ssrd.isel(time=s).values\n",
    "        x_t2m  = self.t2m .isel(time=s).values\n",
    "        x_tp   = self.tp  .isel(time=s).values\n",
    "        # target\n",
    "        y_lai  = self.lai .isel(time=s).values  # may contain NaNs over ocean\n",
    "\n",
    "        # stack to tensors, channel-first\n",
    "        X = np.stack([x_ssrd, x_t2m, x_tp], axis=0)              # (3,H,W)\n",
    "        y = np.expand_dims(y_lai, axis=0)                       # (1,H,W)\n",
    "        mask = ~np.isnan(y)                                     # valid target pixels\n",
    "\n",
    "        # features: fill NaNs with 0 (or choose another fill)\n",
    "        X = np.nan_to_num(X, nan=0.0)\n",
    "\n",
    "        X = torch.from_numpy(X.astype(np.float32))\n",
    "        y = torch.from_numpy(np.nan_to_num(y, nan=0.0).astype(np.float32))\n",
    "        mask = torch.from_numpy(mask.astype(np.bool_))\n",
    "        meta = {\"year\": self.year, \"sample\": int(s)}\n",
    "        return X, y, mask, meta\n",
    "\n",
    "# Loss helper (same as before)\n",
    "def masked_mse_loss(pred, target, mask):\n",
    "    diff2 = (pred - target) ** 2\n",
    "    diff2 = diff2 * mask.float()\n",
    "    denom = mask.float().sum().clamp_min(1.0)\n",
    "    return diff2.sum() / denom\n",
    "\n",
    "def collate_keep_meta(batch):\n",
    "    \"\"\"Custom collate so that 'meta' stays a list of dicts (not collated into tensors).\"\"\"\n",
    "    Xs, ys, masks, metas = zip(*batch)\n",
    "    return torch.stack(Xs), torch.stack(ys), torch.stack(masks), list(metas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e395b665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year=1990, sample=0, loss=166.4673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_113026/1729101369.py:28: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at /pytorch/torch/csrc/autograd/generated/python_variable_methods.cpp:835.)\n",
      "  print(f\"year={meta['year'][0].item()}, sample={meta['sample'][0].item()}, loss={float(loss):.4f}\")\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "ds = ERA5LAIWholeWorld(\n",
    "    year=1990,\n",
    "    era5_mode=\"raw\",   # or \"raw\"/\"z\"\n",
    "    lai_mode=\"raw\",     # or \"anom\"\n",
    "    paths={\n",
    "        \"era5_root\": \"/ptmp/mp002/ellis/lai\",\n",
    "        \"era5_anom_dir\": \"/ptmp/mp040/outputdir/era5/anom\",\n",
    "        \"lai_root\": \"/ptmp/mp002/ellis/lai/lai\",\n",
    "        \"lai_tmpl\": \"LAI.1440.720.{year}.nc\",\n",
    "        # \"lai_anom_dir\": \"/ptmp/mp002/ellis/lai/anom\",  # if using LAI anomalies\n",
    "    },\n",
    ")\n",
    "\n",
    "loader = DataLoader(ds, batch_size=1, shuffle=False)  # batch_size=1 since each item is full globe\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Conv2d(3, 16, 3, padding=1), nn.ReLU(),\n",
    "    nn.Conv2d(16, 1, 3, padding=1)\n",
    ")\n",
    "\n",
    "for X, y, mask, meta in loader:\n",
    "    pred = model(X)\n",
    "    loss = masked_mse_loss(pred, y, mask)\n",
    "    print(f\"year={meta['year'][0].item()}, sample={meta['sample'][0].item()}, loss={float(loss):.4f}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d56c173",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_era5_lai_sequence.py\n",
    "import os, bisect\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def _open_da(path, varname, engine=\"netcdf4\", robust_nan=True):\n",
    "    ds = xr.open_dataset(path, engine=engine)\n",
    "    da = ds[varname]\n",
    "    if robust_nan:\n",
    "        fv = da.attrs.get(\"_FillValue\", None)\n",
    "        if fv is not None:\n",
    "            da = da.where(da != fv)\n",
    "    # north-up\n",
    "    lat_name = \"lat\" if \"lat\" in da.coords else \"latitude\"\n",
    "    da = da.sortby(lat_name)\n",
    "    # attach sample coord\n",
    "    if \"time\" in da.dims and da.sizes[\"time\"] == 24:\n",
    "        da = da.assign_coords(sample=(\"time\", np.arange(24)))\n",
    "    return da\n",
    "\n",
    "def _infer_var(nc_path, engine=\"netcdf4\"):\n",
    "    with xr.open_dataset(nc_path, engine=engine) as ds:\n",
    "        for v in ds.data_vars:\n",
    "            if ds[v].ndim >= 2:\n",
    "                return v\n",
    "    raise RuntimeError(f\"No data variable in {nc_path}\")\n",
    "\n",
    "class ERA5LAISequenceWorld(Dataset):\n",
    "    \"\"\"\n",
    "    Multi-year, whole-world, sliding-window dataset.\n",
    "\n",
    "    Each item:\n",
    "      X: shape depends on feature_layout:\n",
    "         - \"time_channels\": (T*C, H, W)  e.g., (48*3, H, W) for 2 years @ 24/yr\n",
    "         - \"time_first\":    (T, C, H, W) e.g., (48, 3, H, W)\n",
    "      y:\n",
    "         - if target_mode=\"last\": (1, H, W)  (LAI at last timestep in window)\n",
    "         - if target_mode=\"all\":  (T, 1, H, W) (LAI for each timestep)\n",
    "      mask:\n",
    "         - same shape as y; True where target is valid (non-NaN)\n",
    "      meta: dict with {\"start_index\": int, \"years\": [list years spanned], \"t_indices\": [global t indices]}\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    years: list[int]         years to include (e.g., [1982, 1983, ..., 2010])\n",
    "    seq_len: int             window length in samples (e.g., 48 for 2 years)\n",
    "    seq_stride: int          step between window starts (e.g., 12 for quarterly steps)\n",
    "    era5_mode: \"raw\"|\"anom\"|\"z\"\n",
    "    lai_mode:  \"raw\"|\"anom\"\n",
    "    feature_layout: \"time_channels\"|\"time_first\"\n",
    "    target_mode: \"last\"|\"all\"\n",
    "    paths: dict              era5_root, era5_anom_dir, lai_root, lai_tmpl, (optional) lai_anom_dir\n",
    "    engine: str\n",
    "    robust_nan: bool\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        years,\n",
    "        seq_len=48,\n",
    "        seq_stride=24,\n",
    "        era5_mode=\"anom\",\n",
    "        lai_mode=\"raw\",\n",
    "        feature_layout=\"time_channels\",\n",
    "        target_mode=\"last\",\n",
    "        paths=None,\n",
    "        engine=\"netcdf4\",\n",
    "        robust_nan=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert era5_mode in (\"raw\",\"anom\",\"z\")\n",
    "        assert lai_mode in (\"raw\",\"anom\")\n",
    "        assert feature_layout in (\"time_channels\",\"time_first\")\n",
    "        assert target_mode in (\"last\",\"all\")\n",
    "        self.years = list(years)\n",
    "        self.seq_len = int(seq_len)\n",
    "        self.seq_stride = int(seq_stride)\n",
    "        self.era5_mode = era5_mode\n",
    "        self.lai_mode = lai_mode\n",
    "        self.feature_layout = feature_layout\n",
    "        self.target_mode = target_mode\n",
    "        self.engine = engine\n",
    "        self.robust_nan = robust_nan\n",
    "\n",
    "        defaults = {\n",
    "            \"era5_root\": \"/ptmp/mp002/ellis/lai\",\n",
    "            \"era5_anom_dir\": \"/ptmp/mp040/outputdir/era5/anom\",\n",
    "            \"lai_root\": \"/ptmp/mp002/ellis/lai/lai\",\n",
    "            \"lai_tmpl\": \"LAI.1440.720.{year}.nc\",\n",
    "            \"lai_anom_dir\": \"/ptmp/mp002/ellis/lai/anom\",\n",
    "        }\n",
    "        self.paths = defaults if paths is None else {**defaults, **paths}\n",
    "\n",
    "        # Open per-year arrays; keep per-year handles to avoid loading everything at once\n",
    "        self._open_years()\n",
    "\n",
    "        # Build global time index over concatenated years: 24 samples per year\n",
    "        self.samples_per_year = 24\n",
    "        self.year_offsets = [i*self.samples_per_year for i in range(len(self.years))]\n",
    "        self.total_samples = self.samples_per_year * len(self.years)\n",
    "\n",
    "        # Build list of window start indices\n",
    "        self.starts = list(range(0, self.total_samples - self.seq_len + 1, self.seq_stride))\n",
    "        if len(self.starts) == 0:\n",
    "            raise ValueError(\"seq_len is longer than total samples. Reduce seq_len or add years.\")\n",
    "\n",
    "    def _open_years(self):\n",
    "        self.ssrd_y = []\n",
    "        self.t2m_y  = []\n",
    "        self.tp_y   = []\n",
    "        self.lai_y  = []\n",
    "        # open one year to fix dims\n",
    "        example_y = self.years[0]\n",
    "\n",
    "        for y in self.years:\n",
    "            # ERA5 features\n",
    "            if self.era5_mode == \"raw\":\n",
    "                f_ssrd = os.path.join(self.paths[\"era5_root\"], \"ssrd\", f\"ssrd.15daily.fc.era5.1440.720.{y}.nc\")\n",
    "                f_t2m  = os.path.join(self.paths[\"era5_root\"], \"t2m\",  f\"t2m.15daily.an.era5.1440.720.{y}.nc\")\n",
    "                f_tp   = os.path.join(self.paths[\"era5_root\"], \"tp\",   f\"tp.15daily.fc.era5.1440.720.{y}.nc\")\n",
    "                ssrd = _open_da(f_ssrd, \"ssrd\", self.engine, self.robust_nan)\n",
    "                t2m  = _open_da(f_t2m,  \"t2m\",  self.engine, self.robust_nan)\n",
    "                tp   = _open_da(f_tp,   \"tp\",   self.engine, self.robust_nan)\n",
    "            else:\n",
    "                suffix = \"anom\" if self.era5_mode == \"anom\" else \"z\"\n",
    "                base = self.paths[\"era5_anom_dir\"]\n",
    "                ssrd = _open_da(os.path.join(base, f\"ssrd_{suffix}_{y}.nc\"), f\"ssrd_{suffix}\", self.engine, self.robust_nan)\n",
    "                t2m  = _open_da(os.path.join(base, f\"t2m_{suffix}_{y}.nc\"),  f\"t2m_{suffix}\",  self.engine, self.robust_nan)\n",
    "                tp   = _open_da(os.path.join(base, f\"tp_{suffix}_{y}.nc\"),   f\"tp_{suffix}\",   self.engine, self.robust_nan)\n",
    "\n",
    "            # LAI\n",
    "            lai_file = os.path.join(self.paths[\"lai_root\"], self.paths[\"lai_tmpl\"].format(year=y))\n",
    "            lai_var  = _infer_var(lai_file, self.engine)\n",
    "            lai = _open_da(lai_file, lai_var, self.engine, self.robust_nan)\n",
    "            if self.lai_mode == \"anom\":\n",
    "                lai_anom_dir = self.paths.get(\"lai_anom_dir\", self.paths[\"lai_root\"])\n",
    "                lai_anom_file = os.path.join(lai_anom_dir, f\"LAI_anom_{y}.nc\")\n",
    "                if os.path.exists(lai_anom_file):\n",
    "                    lai = _open_da(lai_anom_file, \"LAI_anom\", self.engine, self.robust_nan)\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"LAI anomaly file not found: {lai_anom_file}\")\n",
    "\n",
    "            # basic checks\n",
    "            for da, name in [(ssrd,\"ssrd\"),(t2m,\"t2m\"),(tp,\"tp\")]:\n",
    "                assert \"time\" in da.dims and da.sizes[\"time\"] == 24, f\"{name} {y} expects 24 samples\"\n",
    "\n",
    "            self.ssrd_y.append(ssrd)\n",
    "            self.t2m_y.append(t2m)\n",
    "            self.tp_y.append(tp)\n",
    "            self.lai_y.append(lai)\n",
    "\n",
    "        # dims\n",
    "        self.lat_name = \"lat\" if \"lat\" in self.lai_y[0].coords else \"latitude\"\n",
    "        self.lon_name = \"lon\" if \"lon\" in self.lai_y[0].coords else \"longitude\"\n",
    "        H = int(self.lai_y[0].sizes[self.lat_name]); W = int(self.lai_y[0].sizes[self.lon_name])\n",
    "        for j in range(len(self.years)):\n",
    "            for da, name in [(self.ssrd_y[j],\"ssrd\"),(self.t2m_y[j],\"t2m\"),(self.tp_y[j],\"tp\")]:\n",
    "                assert da.sizes[self.lat_name] == H and da.sizes[self.lon_name] == W, f\"{name} grid mismatch in year {self.years[j]}\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.starts)\n",
    "\n",
    "    def _slice_from_global_t(self, global_t):\n",
    "        \"\"\"Map global sample index to (year_idx, local_time_index).\"\"\"\n",
    "        year_idx = min(len(self.year_offsets)-1, bisect.bisect_right(self.year_offsets, global_t) - 1)\n",
    "        local_t  = global_t - self.year_offsets[year_idx]\n",
    "        return year_idx, local_t\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = self.starts[idx]\n",
    "        Ts = self.seq_len\n",
    "\n",
    "        # Collect sequence across year boundaries\n",
    "        X_list = []   # each entry shape (3, H, W)\n",
    "        y_list = []   # each entry shape (1, H, W) for target if target_mode=\"all\"\n",
    "\n",
    "        for t in range(Ts):\n",
    "            g = start + t\n",
    "            yi, lt = self._slice_from_global_t(g)\n",
    "\n",
    "            # features (H,W)\n",
    "            x_ssrd = self.ssrd_y[yi].isel(time=lt).values\n",
    "            x_t2m  = self.t2m_y [yi].isel(time=lt).values\n",
    "            x_tp   = self.tp_y  [yi].isel(time=lt).values\n",
    "            X_list.append(np.stack([x_ssrd, x_t2m, x_tp], axis=0))  # (3,H,W)\n",
    "\n",
    "            if self.target_mode == \"all\":\n",
    "                y_map = self.lai_y[yi].isel(time=lt).values\n",
    "                y_list.append(np.expand_dims(y_map, axis=0))         # (1,H,W)\n",
    "\n",
    "        # Stack over time\n",
    "        X_seq = np.stack(X_list, axis=0)  # (T, 3, H, W)\n",
    "\n",
    "        # Target\n",
    "        if self.target_mode == \"last\":\n",
    "            glast = start + Ts - 1\n",
    "            yi, lt = self._slice_from_global_t(glast)\n",
    "            y_map = self.lai_y[yi].isel(time=lt).values\n",
    "            y = np.expand_dims(y_map, axis=0)              # (1,H,W)\n",
    "            mask = ~np.isnan(y)\n",
    "        else:\n",
    "            y = np.stack(y_list, axis=0)                   # (T,1,H,W)\n",
    "            mask = ~np.isnan(y)\n",
    "\n",
    "        # Fill NaNs in features, keep y NaNs masked\n",
    "        X_seq = np.nan_to_num(X_seq, nan=0.0)\n",
    "\n",
    "        # Layout\n",
    "        if self.feature_layout == \"time_channels\":\n",
    "            X = X_seq.transpose(1,0,2,3).reshape(-1, X_seq.shape[2], X_seq.shape[3])  # (T*C, H, W)\n",
    "        else:\n",
    "            X = X_seq  # (T, C, H, W)\n",
    "\n",
    "        X_t = torch.from_numpy(X.astype(np.float32))\n",
    "        y_t = torch.from_numpy(np.nan_to_num(y, nan=0.0).astype(np.float32))\n",
    "        m_t = torch.from_numpy(mask.astype(np.bool_))\n",
    "\n",
    "        # meta\n",
    "        # list of global time indices and years covered\n",
    "        t_indices = list(range(start, start+Ts))\n",
    "        years_cov = sorted(set(self.years[self._slice_from_global_t(t)[0]] for t in t_indices))\n",
    "        meta = {\"start_index\": int(start), \"t_indices\": t_indices, \"years\": years_cov}\n",
    "        return X_t, y_t, m_t, meta\n",
    "\n",
    "# Loss (same idea; supports last or all target)\n",
    "def masked_mse_loss(pred, target, mask):\n",
    "    diff2 = (pred - target) ** 2\n",
    "    diff2 = diff2 * mask.float()\n",
    "    denom = mask.float().sum().clamp_min(1.0)\n",
    "    return diff2.sum() / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c34deb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "ds_seq = ERA5LAISequenceWorld(\n",
    "    years=range(1988, 1990),     # training years\n",
    "    seq_len=48,                  # 2 years\n",
    "    seq_stride=24,               # step one year\n",
    "    era5_mode=\"raw\",\n",
    "    lai_mode=\"raw\",\n",
    "    feature_layout=\"time_channels\",  # model input shape (48*3, H, W)\n",
    "    target_mode=\"last\",\n",
    "    paths={\n",
    "        \"era5_root\": \"/ptmp/mp002/ellis/lai\",\n",
    "        \"era5_anom_dir\": \"/ptmp/mp040/outputdir/era5/anom\",\n",
    "        \"lai_root\": \"/ptmp/mp002/ellis/lai/lai\",\n",
    "        \"lai_tmpl\": \"LAI.1440.720.{year}.nc\",\n",
    "    },\n",
    "    engine=\"netcdf4\",\n",
    ")\n",
    "\n",
    "def collate_keep_meta(batch):\n",
    "    Xs, ys, ms, metas = zip(*batch)\n",
    "    return torch.stack(Xs), torch.stack(ys), torch.stack(ms), list(metas)\n",
    "\n",
    "loader = DataLoader(ds_seq, batch_size=1, shuffle=True, collate_fn=collate_keep_meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21158b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Resolve relative to the script location\n",
    "script_dir = os.getcwd()\n",
    "config_path = os.path.join(script_dir, \"..\", \"inputs\", \"training.json\")\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    cfg = json.load(f)\n",
    "\n",
    "train_ds = ERA5LAISequenceWorld(\n",
    "    years=cfg[\"splits\"][\"train_years\"],\n",
    "    seq_len=cfg[\"training\"][\"seq_len_samples\"],\n",
    "    seq_stride=cfg[\"training\"][\"seq_stride\"],\n",
    "    era5_mode=cfg[\"data\"][\"era5_mode\"],\n",
    "    lai_mode=cfg[\"data\"][\"lai_mode\"],\n",
    "    feature_layout=cfg[\"training\"][\"feature_layout\"],\n",
    "    target_mode=cfg[\"training\"][\"target_mode\"],\n",
    "    paths=cfg[\"data\"][\"paths\"],\n",
    "    engine=cfg[\"data\"][\"engine\"],\n",
    ")\n",
    "\n",
    "val_ds = ERA5LAISequenceWorld(\n",
    "    years=cfg[\"splits\"][\"val_years\"],\n",
    "    seq_len=cfg[\"training\"][\"seq_len_samples\"],\n",
    "    seq_stride=cfg[\"training\"][\"seq_stride\"],\n",
    "    era5_mode=cfg[\"data\"][\"era5_mode\"],\n",
    "    lai_mode=cfg[\"data\"][\"lai_mode\"],\n",
    "    feature_layout=cfg[\"training\"][\"feature_layout\"],\n",
    "    target_mode=cfg[\"training\"][\"target_mode\"],\n",
    "    paths=cfg[\"data\"][\"paths\"],\n",
    "    engine=cfg[\"data\"][\"engine\"],\n",
    ")\n",
    "\n",
    "def collate_keep_meta(batch):\n",
    "    Xs, ys, ms, metas = zip(*batch)\n",
    "    return torch.stack(Xs), torch.stack(ys), torch.stack(ms), list(metas)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=cfg[\"training\"][\"batch_size\"],\n",
    "    shuffle=cfg[\"training\"][\"shuffle\"],\n",
    "    collate_fn=collate_keep_meta,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=cfg[\"training\"][\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_keep_meta,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f7c72d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Configuration Loaded ---\n",
      "{\n",
      "  \"splits\": {\n",
      "    \"train_years\": [\n",
      "      1985,\n",
      "      1986,\n",
      "      1987,\n",
      "      1988,\n",
      "      1989,\n",
      "      1990,\n",
      "      1991,\n",
      "      1992,\n",
      "      1993,\n",
      "      1994,\n",
      "      1995,\n",
      "      1996,\n",
      "      1997,\n",
      "      1998,\n",
      "      1999,\n",
      "      2000,\n",
      "      2001,\n",
      "      2002,\n",
      "      2003,\n",
      "      2004,\n",
      "      2005,\n",
      "      2006,\n",
      "      2007,\n",
      "      2008,\n",
      "      2009\n",
      "    ],\n",
      "    \"val_years\": [\n",
      "      2010,\n",
      "      2011,\n",
      "      2012,\n",
      "      2013,\n",
      "      2014,\n",
      "      2015,\n",
      "      2016,\n",
      "      2017\n",
      "    ]\n",
      "  },\n",
      "  \"data\": {\n",
      "    \"era5_mode\": \"raw\",\n",
      "    \"lai_mode\": \"raw\",\n",
      "    \"paths\": {\n",
      "      \"era5_root\": \"/ptmp/mp002/ellis/lai\",\n",
      "      \"era5_anom_dir\": \"/ptmp/mp040/outputdir/era5/anom\",\n",
      "      \"lai_root\": \"/ptmp/mp002/ellis/lai/lai\",\n",
      "      \"lai_tmpl\": \"LAI.1440.720.{year}.nc\"\n",
      "    },\n",
      "    \"engine\": \"netcdf4\"\n",
      "  },\n",
      "  \"training\": {\n",
      "    \"seq_len_samples\": 48,\n",
      "    \"seq_stride\": 24,\n",
      "    \"feature_layout\": \"time_channels\",\n",
      "    \"target_mode\": \"last\",\n",
      "    \"batch_size\": 1,\n",
      "    \"shuffle\": true\n",
      "  }\n",
      "}\n",
      "----------------------------\n",
      "\n",
      "--- Initializing Datasets ---\n",
      "Initializing training dataset...\n",
      "Initializing validation dataset...\n",
      "‚úÖ Training set has 24 samples.\n",
      "‚úÖ Validation set has 7 samples.\n",
      "-----------------------------\n",
      "\n",
      "--- Initializing Model & Device ---\n",
      "‚ö†Ô∏è WARNING: CUDA not available. Using CPU.\n",
      "‚úÖ Model initialized with 144 input channels.\n",
      "---------------------------------\n",
      "\n",
      "--- Starting Training ---\n",
      "Epoch 1/2 [Train] | Batch 24/24 (100%)\n",
      "Epoch 1/2 [Val] | Batch 7/7 (100%)\n",
      "Epoch 1/2 Summary | Train Loss: 30.441332 | Val Loss: 10.397669\n",
      "-------------------------\n",
      "Epoch 2/2 [Train] | Batch 24/24 (100%)\n",
      "Epoch 2/2 [Val] | Batch 7/7 (100%)\n",
      "Epoch 2/2 Summary | Train Loss: 3.180983 | Val Loss: 1.489324\n",
      "-------------------------\n",
      "\n",
      "--- ‚úÖ Training Complete in 1759.30s ---\n",
      "üíæ Saved final model checkpoint to: /raven/ptmp/mp040/ellis-summer-school-group1/pipeline/tinycnn_ckpt.pt\n",
      "\n",
      "--- Loss Progression ---\n",
      "Epoch | Train Loss | Val Loss\n",
      "----------------------------\n",
      "  1   | 30.441332  | 10.397669 \n",
      "  2   | 3.180983   | 1.489324  \n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "# train_optimized.py\n",
    "import os, bisect, json, time\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "# ---------------- Dataset Helper Functions (No changes here) ----------------\n",
    "def _open_da(path, varname, engine=\"netcdf4\", robust_nan=True):\n",
    "    ds = xr.open_dataset(path, engine=engine)\n",
    "    da = ds[varname]\n",
    "    if robust_nan:\n",
    "        fv = da.attrs.get(\"_FillValue\", None)\n",
    "        if fv is not None:\n",
    "            da = da.where(da != fv)\n",
    "    lat_name = \"lat\" if \"lat\" in da.coords else \"latitude\"\n",
    "    da = da.sortby(lat_name)\n",
    "    if \"time\" in da.dims and da.sizes[\"time\"] == 24:\n",
    "        da = da.assign_coords(sample=(\"time\", np.arange(24)))\n",
    "    return da\n",
    "\n",
    "def _infer_var(nc_path, engine=\"netcdf4\"):\n",
    "    with xr.open_dataset(nc_path, engine=engine) as ds:\n",
    "        for v in ds.data_vars:\n",
    "            if ds[v].ndim >= 2:\n",
    "                return v\n",
    "    raise RuntimeError(f\"No suitable data variable found in {nc_path}\")\n",
    "\n",
    "# ---------------- PyTorch Dataset Class (MODIFIED FOR PERFORMANCE) ----------------\n",
    "class ERA5LAISequenceWorld(Dataset):\n",
    "    def __init__(\n",
    "        self, years, seq_len=48, seq_stride=24,\n",
    "        era5_mode=\"anom\", lai_mode=\"raw\",\n",
    "        feature_layout=\"time_channels\", target_mode=\"last\",\n",
    "        paths=None, engine=\"netcdf4\", robust_nan=True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # ... (initial parameters are the same) ...\n",
    "        self.years = list(years)\n",
    "        self.seq_len = int(seq_len)\n",
    "        self.seq_stride = int(seq_stride)\n",
    "        self.era5_mode = era5_mode\n",
    "        self.lai_mode = lai_mode\n",
    "        self.feature_layout = feature_layout\n",
    "        self.target_mode = target_mode\n",
    "        self.engine = engine\n",
    "        self.robust_nan = robust_nan\n",
    "        defaults = {\n",
    "            \"era5_root\": \"/ptmp/mp002/ellis/lai\",\n",
    "            \"era5_anom_dir\": \"/ptmp/mp040/outputdir/era5/anom\",\n",
    "            \"lai_root\": \"/ptmp/mp002/ellis/lai/lai\",\n",
    "            \"lai_tmpl\": \"LAI.1440.720.{year}.nc\",\n",
    "            \"lai_anom_dir\": \"/ptmp/mp002/ellis/lai/anom\",\n",
    "        }\n",
    "        self.paths = defaults if paths is None else {**defaults, **paths}\n",
    "        \n",
    "        # This method now loads all data into RAM for speed\n",
    "        self._open_and_load_years() # <-- MODIFIED\n",
    "\n",
    "        self.samples_per_year = 24\n",
    "        self.total_samples = self.lai_data.shape[0] # <-- MODIFIED to use the loaded data shape\n",
    "\n",
    "        self.starts = list(range(0, self.total_samples - self.seq_len + 1, self.seq_stride))\n",
    "        if not self.starts:\n",
    "            raise ValueError(\"seq_len is longer than total samples. Reduce seq_len or add years.\")\n",
    "\n",
    "    def _open_and_load_years(self): # <-- MODIFIED: New optimized loading function\n",
    "        \"\"\"\n",
    "        Loads all necessary data for the given years, concatenates them,\n",
    "        and then loads everything into memory as NumPy arrays to prevent I/O bottlenecks.\n",
    "        \"\"\"\n",
    "        ssrd_y, t2m_y, tp_y, lai_y = [], [], [], []\n",
    "        print(f\"    - Loading data for years: {self.years}...\") # <-- NEW progress indicator\n",
    "        for y in self.years:\n",
    "            # ... (the logic to find and open individual files is the same) ...\n",
    "            if self.era5_mode == \"raw\":\n",
    "                f_ssrd = os.path.join(self.paths[\"era5_root\"], \"ssrd\", f\"ssrd.15daily.fc.era5.1440.720.{y}.nc\")\n",
    "                f_t2m  = os.path.join(self.paths[\"era5_root\"], \"t2m\",  f\"t2m.15daily.an.era5.1440.720.{y}.nc\")\n",
    "                f_tp   = os.path.join(self.paths[\"era5_root\"], \"tp\",   f\"tp.15daily.fc.era5.1440.720.{y}.nc\")\n",
    "                ssrd = _open_da(f_ssrd, \"ssrd\", self.engine, self.robust_nan)\n",
    "                t2m  = _open_da(f_t2m,  \"t2m\",  self.engine, self.robust_nan)\n",
    "                tp   = _open_da(f_tp,   \"tp\",   self.engine, self.robust_nan)\n",
    "            else:\n",
    "                suffix = \"anom\" if self.era5_mode == \"anom\" else \"z\"\n",
    "                base = self.paths[\"era5_anom_dir\"]\n",
    "                ssrd = _open_da(os.path.join(base, f\"ssrd_{suffix}_{y}.nc\"), f\"ssrd_{suffix}\", self.engine, self.robust_nan)\n",
    "                t2m  = _open_da(os.path.join(base, f\"t2m_{suffix}_{y}.nc\"),  f\"t2m_{suffix}\",  self.engine, self.robust_nan)\n",
    "                tp   = _open_da(os.path.join(base, f\"tp_{suffix}_{y}.nc\"),   f\"tp_{suffix}\",   self.engine, self.robust_nan)\n",
    "            lai_file = os.path.join(self.paths[\"lai_root\"], self.paths[\"lai_tmpl\"].format(year=y))\n",
    "            lai_var  = _infer_var(lai_file, self.engine)\n",
    "            lai = _open_da(lai_file, lai_var, self.engine, self.robust_nan)\n",
    "            if self.lai_mode == \"anom\":\n",
    "                lai_anom_file = os.path.join(self.paths.get(\"lai_anom_dir\", self.paths[\"lai_root\"]), f\"LAI_anom_{y}.nc\")\n",
    "                lai = _open_da(lai_anom_file, \"LAI_anom\", self.engine, self.robust_nan)\n",
    "            \n",
    "            ssrd_y.append(ssrd); t2m_y.append(t2m); tp_y.append(tp); lai_y.append(lai)\n",
    "        \n",
    "        # --- NEW OPTIMIZATION STEP ---\n",
    "        # Concatenate all years into single xarray DataArrays\n",
    "        print(\"    - Concatenating yearly data...\")\n",
    "        full_ssrd = xr.concat(ssrd_y, dim=\"time\")\n",
    "        full_t2m  = xr.concat(t2m_y, dim=\"time\")\n",
    "        full_tp   = xr.concat(tp_y, dim=\"time\")\n",
    "        full_lai  = xr.concat(lai_y, dim=\"time\")\n",
    "\n",
    "        # Now, load all the data from disk into RAM. This is the key performance gain.\n",
    "        # This will be slow once, but makes training much faster.\n",
    "        print(\"    - Loading all data into memory (this may take a moment)...\")\n",
    "        self.ssrd_data = full_ssrd.load().values\n",
    "        self.t2m_data  = full_t2m.load().values\n",
    "        self.tp_data   = full_tp.load().values\n",
    "        self.lai_data  = full_lai.load().values\n",
    "        print(\"    - ‚úÖ Data loaded.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.starts)\n",
    "\n",
    "    def __getitem__(self, idx): # <-- MODIFIED: New optimized __getitem__\n",
    "        \"\"\"Fetches a single training sequence by slicing the pre-loaded NumPy arrays.\"\"\"\n",
    "        start = self.starts[idx]\n",
    "        end = start + self.seq_len\n",
    "\n",
    "        # --- SLICE pre-loaded NumPy arrays (extremely fast) ---\n",
    "        # Shape of each is (T, H, W) where T=seq_len\n",
    "        x_ssrd_seq = self.ssrd_data[start:end, :, :]\n",
    "        x_t2m_seq  = self.t2m_data[start:end, :, :]\n",
    "        x_tp_seq   = self.tp_data[start:end, :, :]\n",
    "\n",
    "        # Stack the features to get (T, 3, H, W)\n",
    "        X_seq = np.stack([x_ssrd_seq, x_t2m_seq, x_tp_seq], axis=1)\n",
    "\n",
    "        # --- Prepare the target (y) and mask from pre-loaded data ---\n",
    "        if self.target_mode == \"last\":\n",
    "            # Target is the LAI at the final time step of the sequence\n",
    "            y = self.lai_data[end - 1, :, :] # Get the last slice\n",
    "            y = np.expand_dims(y, axis=0)    # Add channel dim -> (1, H, W)\n",
    "            mask = ~np.isnan(y)\n",
    "        else: # target_mode == \"all\"\n",
    "            y = self.lai_data[start:end, :, :]\n",
    "            y = np.expand_dims(y, axis=1) # Add channel dim -> (T, 1, H, W)\n",
    "            mask = ~np.isnan(y)\n",
    "        \n",
    "        X_seq = np.nan_to_num(X_seq, nan=0.0)\n",
    "\n",
    "        # --- Reshape features based on layout (no changes here) ---\n",
    "        if self.feature_layout == \"time_channels\":\n",
    "            X = X_seq.transpose(0, 2, 3, 1).reshape(X_seq.shape[2], X_seq.shape[3], -1)\n",
    "            X = X.transpose(2, 0, 1) # to get C,H,W -> (3T, H, W)\n",
    "        else:\n",
    "            X = X_seq\n",
    "\n",
    "        # --- Convert to PyTorch tensors (no changes here) ---\n",
    "        X_t = torch.from_numpy(X.astype(np.float32))\n",
    "        y_t = torch.from_numpy(np.nan_to_num(y, nan=0.0).astype(np.float32))\n",
    "        m_t = torch.from_numpy(mask.astype(np.bool_))\n",
    "\n",
    "        # We can simplify the metadata as we don't need to track years anymore\n",
    "        meta = {\"start_index\": int(start)}\n",
    "        return X_t, y_t, m_t, meta\n",
    "\n",
    "# ... (The rest of the script, including TinyCNN, collate_fn, and the main training loop,\n",
    "#      remains exactly the same as the previous version with all the print statements.) ...\n",
    "\n",
    "def masked_mse_loss(pred, target, mask):\n",
    "    diff2 = (pred - target) ** 2\n",
    "    diff2 = diff2 * mask.float()\n",
    "    denom = mask.float().sum().clamp_min(1.0)\n",
    "    return diff2.sum() / denom\n",
    "\n",
    "def collate_keep_meta(batch):\n",
    "    Xs, ys, ms, metas = zip(*batch)\n",
    "    return torch.stack(Xs), torch.stack(ys), torch.stack(ms), list(metas)\n",
    "\n",
    "class TinyCNN(nn.Module):\n",
    "    def __init__(self, in_ch, mid=16, out_ch=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, mid, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid, mid, kernel_size=3, padding=1),   nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid, out_ch, kernel_size=1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        script_dir = os.getcwd()\n",
    "    config_path = os.path.join(script_dir, \"..\", \"inputs\", \"training.json\")\n",
    "    with open(config_path, \"r\") as f:\n",
    "        cfg = json.load(f)\n",
    "    print(\"--- Configuration Loaded ---\")\n",
    "    print(json.dumps(cfg, indent=2))\n",
    "    print(\"-\" * 28)\n",
    "    print(\"\\n--- Initializing Datasets ---\")\n",
    "    print(\"Initializing training dataset...\")\n",
    "    train_ds = ERA5LAISequenceWorld(\n",
    "        years=cfg[\"splits\"][\"train_years\"],\n",
    "        seq_len=cfg[\"training\"][\"seq_len_samples\"],\n",
    "        seq_stride=cfg[\"training\"][\"seq_stride\"],\n",
    "        era5_mode=cfg[\"data\"][\"era5_mode\"],\n",
    "        lai_mode=cfg[\"data\"][\"lai_mode\"],\n",
    "        feature_layout=cfg[\"training\"][\"feature_layout\"],\n",
    "        target_mode=cfg[\"training\"][\"target_mode\"],\n",
    "        paths=cfg[\"data\"][\"paths\"],\n",
    "        engine=cfg[\"data\"][\"engine\"],\n",
    "    )\n",
    "    print(\"Initializing validation dataset...\")\n",
    "    val_ds = ERA5LAISequenceWorld(\n",
    "        years=cfg[\"splits\"][\"val_years\"],\n",
    "        seq_len=cfg[\"training\"][\"seq_len_samples\"],\n",
    "        seq_stride=cfg[\"training\"][\"seq_stride\"],\n",
    "        era5_mode=cfg[\"data\"][\"era5_mode\"],\n",
    "        lai_mode=cfg[\"data\"][\"lai_mode\"],\n",
    "        feature_layout=cfg[\"training\"][\"feature_layout\"],\n",
    "        target_mode=cfg[\"training\"][\"target_mode\"],\n",
    "        paths=cfg[\"data\"][\"paths\"],\n",
    "        engine=cfg[\"data\"][\"engine\"],\n",
    "    )\n",
    "    print(f\"‚úÖ Training set has {len(train_ds)} samples.\")\n",
    "    print(f\"‚úÖ Validation set has {len(val_ds)} samples.\")\n",
    "    print(\"-\" * 29)\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=cfg[\"training\"][\"batch_size\"],\n",
    "        shuffle=cfg[\"training\"][\"shuffle\"], num_workers=0,\n",
    "        collate_fn=collate_keep_meta, pin_memory=True # Can set pin_memory=True now\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_ds, batch_size=cfg[\"training\"][\"batch_size\"],\n",
    "        shuffle=False, num_workers=0,\n",
    "        collate_fn=collate_keep_meta, pin_memory=True # Can set pin_memory=True now\n",
    "    )\n",
    "    print(\"\\n--- Initializing Model & Device ---\")\n",
    "    DEVICE_ID = 1\n",
    "    if torch.cuda.is_available():\n",
    "        if DEVICE_ID >= torch.cuda.device_count():\n",
    "            print(f\"‚ö†Ô∏è WARNING: Device ID {DEVICE_ID} is not available. Found {torch.cuda.device_count()} devices.\")\n",
    "            DEVICE_ID = 0\n",
    "        device = torch.device(f\"cuda:{DEVICE_ID}\")\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        gpu_name = torch.cuda.get_device_name(device)\n",
    "        print(f\"‚úÖ Using GPU: {gpu_name} (Device {DEVICE_ID})\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"‚ö†Ô∏è WARNING: CUDA not available. Using CPU.\")\n",
    "    X0, _, _, _ = next(iter(train_loader))\n",
    "    in_channels = X0.shape[1]\n",
    "    model = TinyCNN(in_ch=in_channels)\n",
    "    model.to(device)\n",
    "    print(f\"‚úÖ Model initialized with {in_channels} input channels.\")\n",
    "    print(\"-\" * 33)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scaler = GradScaler() if device.type == \"cuda\" else None\n",
    "    EPOCHS = 2\n",
    "    ckpt_path = os.path.join(script_dir, \"tinycnn_ckpt.pt\")\n",
    "    loss_history = {\"train\": [], \"val\": []}\n",
    "    def epoch_loop(loader, train=True, epoch_num=0, total_epochs=0):\n",
    "        model.train(train)\n",
    "        total_loss, n_batches = 0.0, 0\n",
    "        mode = \"Train\" if train else \"Val\"\n",
    "        loader_len = len(loader)\n",
    "        for batch_idx, (X, y, mask, metas) in enumerate(loader):\n",
    "            non_blocking = (device.type == \"cuda\")\n",
    "            X = X.to(device, non_blocking=non_blocking)\n",
    "            y = y.to(device, non_blocking=non_blocking)\n",
    "            mask = mask.to(device, non_blocking=non_blocking)\n",
    "            with torch.set_grad_enabled(train):\n",
    "                with autocast(device_type=device.type, enabled=(scaler is not None)):\n",
    "                    pred = model(X)\n",
    "                    loss = masked_mse_loss(pred, y, mask)\n",
    "                if train:\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "                    if scaler is not None:\n",
    "                        scaler.scale(loss).backward()\n",
    "                        scaler.step(opt)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        loss.backward()\n",
    "                        opt.step()\n",
    "            total_loss += float(loss.detach())\n",
    "            n_batches += 1\n",
    "            if (batch_idx + 1) % 20 == 0 or (batch_idx + 1) == loader_len:\n",
    "                progress = (batch_idx + 1) / loader_len\n",
    "                print(f\"\\rEpoch {epoch_num}/{total_epochs} [{mode}] | Batch {batch_idx+1}/{loader_len} ({progress:.0%})\", end=\"\")\n",
    "        print()\n",
    "        return total_loss / max(n_batches, 1)\n",
    "    print(\"\\n--- Starting Training ---\")\n",
    "    start_time = time.time()\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        train_loss = epoch_loop(train_loader, train=True, epoch_num=epoch, total_epochs=EPOCHS)\n",
    "        loss_history[\"train\"].append(train_loss)\n",
    "        val_loss = epoch_loop(val_loader, train=False, epoch_num=epoch, total_epochs=EPOCHS)\n",
    "        loss_history[\"val\"].append(val_loss)\n",
    "        print(f\"Epoch {epoch}/{EPOCHS} Summary | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f}\")\n",
    "        print(\"-\" * 25)\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n--- ‚úÖ Training Complete in {total_time:.2f}s ---\")\n",
    "    torch.save({\"model\": model.state_dict(), \"in_channels\": in_channels}, ckpt_path)\n",
    "    print(f\"üíæ Saved final model checkpoint to: {ckpt_path}\")\n",
    "    print(\"\\n--- Loss Progression ---\")\n",
    "    print(\"Epoch | Train Loss | Val Loss\")\n",
    "    print(\"----------------------------\")\n",
    "    for i in range(EPOCHS):\n",
    "        print(f\"{i+1:^5} | {loss_history['train'][i]:<10.6f} | {loss_history['val'][i]:<10.6f}\")\n",
    "    print(\"----------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ellis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
